{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb158ef",
   "metadata": {},
   "source": [
    "# Lecture 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aa0d0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('diabetes.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f1ce2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data_array = data.to_numpy()\n",
    "data_labels = data_array[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "669205a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 8)\n",
      "[[-0.25095213  0.28497518  0.45982725 ... -0.4559673  -0.4496241\n",
      "   0.06459135]\n",
      " [-1.14185152  1.84983245  0.45982725 ...  3.4785293   5.88356477\n",
      "  -0.70119842]\n",
      " [ 0.93691372  1.19259239 -0.26394125 ... -0.58288655 -0.53720754\n",
      "   0.57511787]\n",
      " ...\n",
      " [-0.84488505  0.72313521  0.66661825 ...  1.18129096  0.4080896\n",
      "  -0.44593516]\n",
      " [-0.25095213  1.66204957 -0.57412775 ...  0.1151693   0.36580794\n",
      "   0.23476686]\n",
      " [ 0.04601433 -0.68523633 -0.05715025 ...  0.10247738 -0.98720523\n",
      "  -0.0204964 ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.shuffle(data_array)\n",
    "\n",
    "data_features = data_array[:, :-1]\n",
    "data_labels = data_array[:, -1]\n",
    "\n",
    "standard_scaler = StandardScaler()\n",
    "normalized_data_features = standard_scaler.fit_transform(data_features)\n",
    "\n",
    "print(normalized_data_features.shape)\n",
    "print(normalized_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a641a678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 1.\n",
      " 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 0. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.\n",
      " 1. 1. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1. 1.\n",
      " 0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 1. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0.\n",
      " 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 0.\n",
      " 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 1.\n",
      " 0. 1. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 1.\n",
      " 0. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 1. 0. 0. 1. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 1.\n",
      " 0. 1. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      " 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 1. 0. 0. 1.\n",
      " 0. 1. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1.\n",
      " 0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(data_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b388317b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " ...\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "one_hot_labels = OneHotEncoder(max_categories=2, sparse=False).fit_transform(np.reshape(data_labels, (-1, 1)))\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13dd5efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_size = int(len(data_array) * 0.7)\n",
    "\n",
    "train_data = normalized_data_features[:training_data_size]\n",
    "test_data = normalized_data_features[training_data_size:]\n",
    "\n",
    "train_data_x = normalized_data_features[:training_data_size]\n",
    "train_data_y = one_hot_labels[:training_data_size]\n",
    "\n",
    "test_data_x = normalized_data_features[training_data_size:]\n",
    "test_data_y = one_hot_labels[training_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cf4cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:31:46.653039: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-16 16:31:46.826878: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-02-16 16:31:46.831890: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:31:46.831909: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-16 16:31:47.560256: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:31:47.560344: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:31:47.560351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c88e8",
   "metadata": {},
   "source": [
    "## Building a sequential model using Keras API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a5229b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 1024)              9216      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-16 16:31:48.703242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-16 16:31:48.703287: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-16 16:31:48.703319: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PSLPT514): /proc/driver/nvidia/version does not exist\n",
      "2023-02-16 16:31:48.703998: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) # the shape of the input, in this example the number of features is 8\n",
    "model.add(keras.layers.Dense(1024, activation='sigmoid'))\n",
    "# the number of neurons in the outputs have to be the same as the size of the one-hot vector\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ce4192c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2851392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 1s 2ms/step - loss: 10.1889\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 4.9195\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 3.4340\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.1400\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.6068\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3177\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.7377\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 2.0136\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.4145\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3980\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.0476\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.1379\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.2644\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.0999\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.0311\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7931\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7888\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7705\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.8568\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6968\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6933\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6851\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7001\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8447\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6249\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6558\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5607\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6179\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5163\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6427\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5380\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4875\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5966\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5073\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4849\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5266\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5042\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5530\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5418\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4925\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5294\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5275\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4966\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4802\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5045\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5166\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4874\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5040\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4899\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5055\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4798\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4909\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4435\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4456\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4992\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4611\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4406\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4481\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4325\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4447\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4773\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4454\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4399\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4383\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4123\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4165\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4247\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4433\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4090\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4107\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4128\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3966\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3947\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4078\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4088\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4358\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4055\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3964\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4139\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4125\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4278\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3968\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5015\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4090\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4050\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4173\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3998\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3961\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4123\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3791\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4184\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3881\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3815\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3822\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3858\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3785\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3823\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3705\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3819\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3742\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3675\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3781\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3695\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3953\n",
      "Epoch 105/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3749\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3853\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3626\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3986\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3777\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3625\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3548\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3530\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3391\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3697\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3571\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3558\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3484\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3463\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3484\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3556\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3525\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3421\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3445\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3381\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3560\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3437\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3508\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3459\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3386\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3308\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3253\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3306\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3309\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3326\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3185\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3217\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3247\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3149\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3109\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3278\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2995\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3185\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3195\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3109\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3148\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3105\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3033\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3046\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3003\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3184\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2936\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3053\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2984\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2926\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2940\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2916\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2880\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2825\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2876\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2865\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3269\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3012\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2698\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2952\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2702\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2713\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2682\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2698\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2670\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2899\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2781\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2668\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2605\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2575\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2633\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2539\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2545\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2453\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.2451\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2529\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2383\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2513\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2531\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2326\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2415\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2297\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3046\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2438\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2330\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2409\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2569\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2377\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2287\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2216\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2345\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2233\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2236\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2233\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2162\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd330f37c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data_x, train_data_y, batch_size=8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39978045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41fb8d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def print_metrics(predicted_y, data_y, threshold=0.5):\n",
    "    predicted_label = predicted_y >= threshold\n",
    "    true_label = data_y >= threshold\n",
    "    acc = accuracy_score(true_label, predicted_label)\n",
    "    print(f'accuracy (at threshold {threshold}): {acc}')\n",
    "\n",
    "    confusion = confusion_matrix(true_label, predicted_label)\n",
    "    print('')\n",
    "    print('confusion matrix:')\n",
    "    print(confusion)\n",
    "\n",
    "    fnr = confusion[1, 0] / (confusion[0, 0] + confusion[1, 0])\n",
    "    fpr = confusion[0, 1] / (confusion[0, 1] + confusion[1, 1])\n",
    "    print('')\n",
    "    print(f'FNR: {fnr}')\n",
    "    print(f'FPR: {fpr}')\n",
    "\n",
    "    roc_fpr, roc_tpr, roc_thresholds = roc_curve(data_y, predicted_y)\n",
    "    roc_auc = auc(roc_fpr, roc_tpr)\n",
    "\n",
    "    print('')\n",
    "    print(f'auc: {roc_auc}')\n",
    "\n",
    "    precision, recall, fscore, _ = precision_recall_fscore_support(true_label, predicted_label, average='binary')\n",
    "\n",
    "    print('')\n",
    "    print(f'precision: {precision}')\n",
    "    print(f'recall: {recall}')\n",
    "    print(f'fscore: {fscore}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "717436df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.9124767225325885\n",
      "\n",
      "confusion matrix:\n",
      "[[333   9]\n",
      " [ 38 157]]\n",
      "\n",
      "FNR: 0.10242587601078167\n",
      "FPR: 0.05421686746987952\n",
      "\n",
      "auc: 0.9747488379067326\n",
      "\n",
      "precision: 0.9457831325301205\n",
      "recall: 0.8051282051282052\n",
      "fscore: 0.8698060941828255\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.7316017316017316\n",
      "\n",
      "confusion matrix:\n",
      "[[121  37]\n",
      " [ 25  48]]\n",
      "\n",
      "FNR: 0.17123287671232876\n",
      "FPR: 0.43529411764705883\n",
      "\n",
      "auc: 0.7630483787064332\n",
      "\n",
      "precision: 0.5647058823529412\n",
      "recall: 0.6575342465753424\n",
      "fscore: 0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842afcbf",
   "metadata": {},
   "source": [
    "## Adding Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51ae2cfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_2 (Dense)             (None, 1024)              9216      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) \n",
    "model.add(keras.layers.Dense(1024, activation='sigmoid'))\n",
    "model.add(keras.layers.Dropout(0.5)) # half of the neurons in the previous layer will be dropped randomly\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()\n",
    "68/68 [==============================] - 0s 1ms/step - loss: 0.4123\n",
    "Epoch 90/200\n",
    "68/68 [==============================] - 0s 1ms/step - loss: 0.3791\n",
    "Epoch 91/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95106d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 7.8230\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 4.8329\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 5.3581\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 4.5049\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 3.5285\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 3.7294\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 3.7678\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.9200\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.4341\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.1128\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.7091\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 2.1919\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.7624\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.9539\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.8719\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2245\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.6119\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3367\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 1.3772\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3338\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.3132\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.0466\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 1.2774\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9463\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.9458\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.9849\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.9982\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9004\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.9051\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7810\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7445\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.8035\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.8932\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7788\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7915\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7382\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6207\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6360\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6076\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5641\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.8032\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6855\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6667\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6286\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.7126\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5933\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5660\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5596\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5580\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5598\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5034\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5204\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5769\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6006\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5144\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4918\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5306\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5420\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5306\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5108\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4614\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5098\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4607\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4949\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5046\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5129\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4691\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4809\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5158\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4628\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5115\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4720\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4820\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4493\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4858\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4669\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4566\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4729\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4604\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5468\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4921\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4770\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4859\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4721\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4323\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4815\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4666\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4533\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4735\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4620\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4772\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4368\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4590\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4346\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4451\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4195\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4435\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4712\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4076\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4138\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4186\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4712\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4715\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4190\n",
      "Epoch 105/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4020\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4121\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4022\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3881\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4196\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3844\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4080\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3631\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4687\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4173\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4054\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4096\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4633\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4145\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3941\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3895\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4131\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4022\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4788\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4166\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3850\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3967\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3919\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3920\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4099\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4270\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3982\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4507\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4188\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4114\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4037\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3990\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3861\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3949\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3683\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3935\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3652\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4263\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3710\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4060\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4045\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4027\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4150\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4390\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3741\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3945\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3852\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3793\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3967\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3981\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3900\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3681\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3861\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3647\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4167\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3472\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3646\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3811\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3873\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3744\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3616\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3988\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3683\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3562\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3690\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4032\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3874\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3867\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3542\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3843\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3769\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3706\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3740\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3620\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3599\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3686\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3469\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3801\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3664\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3702\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3460\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3603\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3182\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3656\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3658\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3717\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3424\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3835\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3769\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3308\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3478\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3556\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3497\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3500\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd3061b640>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=1.0))\n",
    "model.fit(train_data_x, train_data_y, batch_size=8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd7c2b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.8901303538175046\n",
      "\n",
      "confusion matrix:\n",
      "[[307  35]\n",
      " [ 24 171]]\n",
      "\n",
      "FNR: 0.07250755287009064\n",
      "FPR: 0.16990291262135923\n",
      "\n",
      "auc: 0.9552406657669815\n",
      "\n",
      "precision: 0.8300970873786407\n",
      "recall: 0.8769230769230769\n",
      "fscore: 0.8528678304239402\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.683982683982684\n",
      "\n",
      "confusion matrix:\n",
      "[[106  52]\n",
      " [ 21  52]]\n",
      "\n",
      "FNR: 0.16535433070866143\n",
      "FPR: 0.5\n",
      "\n",
      "auc: 0.7662562857638286\n",
      "\n",
      "precision: 0.5\n",
      "recall: 0.7123287671232876\n",
      "fscore: 0.5875706214689266\n"
     ]
    }
   ],
   "source": [
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)\n",
    "\n",
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6436bf97",
   "metadata": {},
   "source": [
    "## Using ReLU in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4f5bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 1024)              9216      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) \n",
    "model.add(keras.layers.Dense(1024, activation='relu'))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "03ea5343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6682\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6225\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5915\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5633\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5456\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5240\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5165\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5002\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4913\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4862\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4769\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4745\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4700\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4619\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4573\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4535\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4500\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4442\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4421\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4446\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4450\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4445\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4426\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4361\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4377\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4391\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4348\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4322\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4350\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4399\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4365\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4358\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4299\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4326\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4321\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4324\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4286\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4240\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4289\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4281\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4287\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4326\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4212\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4307\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4222\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4225\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4214\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4171\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4205\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4236\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4265\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4180\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4175\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4209\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4155\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4156\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4164\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4218\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4128\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4114\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4167\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4151\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4134\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4153\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4141\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4163\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4117\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4091\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4164\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4052\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4121\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4128\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4116\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4053\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4085\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4065\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4059\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4055\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4048\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4082\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4106\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4112\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4034\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4114\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4037\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4030\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4029\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4059\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4028\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3946\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4005\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4018\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3989\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3981\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4030\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3995\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3951\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3974\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3977\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3940\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3972\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4003\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3949\n",
      "Epoch 105/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3993\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3931\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3997\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3959\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3991\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3957\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3938\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3976\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3969\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3902\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3873\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3912\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3831\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3917\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3998\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3921\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3942\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3966\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3979\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3890\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3851\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3877\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3851\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3904\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3939\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3916\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3879\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3872\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3889\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3878\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3868\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3893\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3873\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3861\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3912\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3865\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3882\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3865\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3787\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3834\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3848\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3883\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3841\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3886\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3900\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3858\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3865\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3767\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3829\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3863\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3876\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3828\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3864\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3840\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3880\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3858\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3867\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3839\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3800\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3799\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3856\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3841\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3823\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3810\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3798\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3721\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3765\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3813\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3724\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3727\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3774\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3725\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3693\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3748\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3759\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3777\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3811\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3694\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3738\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3752\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3795\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3696\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3791\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3740\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3690\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3730\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3808\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3725\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3727\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3838\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3652\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3789\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3753\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3711\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3721\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbd204be6b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01))\n",
    "model.fit(train_data_x, train_data_y, batch_size=8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13f6fb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 2ms/step\n",
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.8528864059590316\n",
      "\n",
      "confusion matrix:\n",
      "[[308  34]\n",
      " [ 45 150]]\n",
      "\n",
      "FNR: 0.1274787535410765\n",
      "FPR: 0.18478260869565216\n",
      "\n",
      "auc: 0.9123706702654072\n",
      "\n",
      "precision: 0.8152173913043478\n",
      "recall: 0.7692307692307693\n",
      "fscore: 0.79155672823219\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.7142857142857143\n",
      "\n",
      "confusion matrix:\n",
      "[[117  41]\n",
      " [ 25  48]]\n",
      "\n",
      "FNR: 0.176056338028169\n",
      "FPR: 0.4606741573033708\n",
      "\n",
      "auc: 0.788624934974857\n",
      "\n",
      "precision: 0.5393258426966292\n",
      "recall: 0.6575342465753424\n",
      "fscore: 0.5925925925925926\n"
     ]
    }
   ],
   "source": [
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)\n",
    "\n",
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d53df0",
   "metadata": {},
   "source": [
    "## Using Leaky ReLU in hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58b532cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 1024)              9216      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) \n",
    "model.add(keras.layers.Dense(1024, activation=None))\n",
    "model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd69d852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6719\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.6200\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5889\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5636\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5433\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5233\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5113\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4990\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4920\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4833\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4766\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4711\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4650\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4594\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4623\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4553\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4530\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4504\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4508\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4471\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4457\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4482\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4433\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4381\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4392\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4420\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4396\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4311\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4355\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4354\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4345\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4317\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4339\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4312\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4318\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4321\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4355\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4252\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4288\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4279\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4252\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4255\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4283\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4289\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4274\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4233\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4255\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4206\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4223\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4212\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4215\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4164\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4200\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4209\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4218\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4251\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4170\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4224\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4215\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4173\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4192\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4169\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4161\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4160\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4135\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4143\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4117\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4184\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4095\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4169\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4137\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4198\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4103\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4177\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4131\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4130\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4134\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4135\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4121\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4069\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4089\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4144\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4122\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4062\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4058\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4061\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4030\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4066\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4090\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4039\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4045\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4144\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4062\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4018\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4074\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4051\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4065\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4025\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4024\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3996\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4016\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4027\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4015\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3986\n",
      "Epoch 105/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3961\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3995\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4015\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3997\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4033\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3992\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4035\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4053\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3940\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3963\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4034\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3985\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4034\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3995\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3919\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3968\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3948\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3948\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3928\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4022\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3924\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4002\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3975\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3912\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4027\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3956\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3915\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3905\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4004\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3928\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3943\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3920\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3916\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3923\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3894\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3893\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3924\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3849\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3880\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3931\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3898\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3890\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3878\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3898\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3890\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3854\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3844\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3907\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3840\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3883\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3782\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3923\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3834\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3846\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3780\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3913\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3843\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3868\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3858\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3753\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3813\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3842\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3835\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3796\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3900\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3829\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3815\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3836\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3825\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3828\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3790\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3724\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3795\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3820\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3790\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3819\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3822\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3740\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3814\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3821\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3808\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3846\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3787\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3819\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3769\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3858\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3819\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3805\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3713\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3840\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3797\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3755\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3704\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3768\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3777\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbcfc760730>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.SGD(learning_rate=0.01))\n",
    "model.fit(train_data_x, train_data_y, batch_size=8, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9bd896d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.8566108007448789\n",
      "\n",
      "confusion matrix:\n",
      "[[311  31]\n",
      " [ 46 149]]\n",
      "\n",
      "FNR: 0.12885154061624648\n",
      "FPR: 0.17222222222222222\n",
      "\n",
      "auc: 0.9086969560653771\n",
      "\n",
      "precision: 0.8277777777777777\n",
      "recall: 0.764102564102564\n",
      "fscore: 0.7946666666666665\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.7316017316017316\n",
      "\n",
      "confusion matrix:\n",
      "[[121  37]\n",
      " [ 25  48]]\n",
      "\n",
      "FNR: 0.17123287671232876\n",
      "FPR: 0.43529411764705883\n",
      "\n",
      "auc: 0.7895786370730015\n",
      "\n",
      "precision: 0.5647058823529412\n",
      "recall: 0.6575342465753424\n",
      "fscore: 0.6075949367088608\n"
     ]
    }
   ],
   "source": [
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)\n",
    "\n",
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fafe4",
   "metadata": {},
   "source": [
    "## Model Valdiation and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15939764",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_size = int(len(data_array) * 0.7)\n",
    "validation_data_size = int(len(data_array) * 0.15)\n",
    "\n",
    "train_data_x = normalized_data_features[:training_data_size]\n",
    "train_data_y = one_hot_labels[:training_data_size]\n",
    "\n",
    "validation_data_x = normalized_data_features[training_data_size:training_data_size + validation_data_size]\n",
    "validation_data_y = one_hot_labels[training_data_size:training_data_size + validation_data_size]\n",
    "\n",
    "test_data_x = normalized_data_features[training_data_size + validation_data_size:]\n",
    "test_data_y = one_hot_labels[training_data_size + validation_data_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9983a429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             (None, 1024)              9216      \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) \n",
    "model.add(keras.layers.Dense(1024, activation=None))\n",
    "model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd2497e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 1s 5ms/step - loss: 0.6739 - binary_accuracy: 0.6173 - val_loss: 0.6519 - val_binary_accuracy: 0.7043\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.6207 - binary_accuracy: 0.7374 - val_loss: 0.6185 - val_binary_accuracy: 0.7304\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.5895 - binary_accuracy: 0.7570 - val_loss: 0.5966 - val_binary_accuracy: 0.7348\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5592 - binary_accuracy: 0.7709 - val_loss: 0.5839 - val_binary_accuracy: 0.7435\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5462 - binary_accuracy: 0.7682 - val_loss: 0.5722 - val_binary_accuracy: 0.7391\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.5268 - binary_accuracy: 0.7784 - val_loss: 0.5630 - val_binary_accuracy: 0.7391\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.5118 - binary_accuracy: 0.7840 - val_loss: 0.5563 - val_binary_accuracy: 0.7435\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4995 - binary_accuracy: 0.7942 - val_loss: 0.5518 - val_binary_accuracy: 0.7435\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4908 - binary_accuracy: 0.7886 - val_loss: 0.5497 - val_binary_accuracy: 0.7348\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4845 - binary_accuracy: 0.7821 - val_loss: 0.5483 - val_binary_accuracy: 0.7348\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4816 - binary_accuracy: 0.7877 - val_loss: 0.5479 - val_binary_accuracy: 0.7304\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4696 - binary_accuracy: 0.7961 - val_loss: 0.5470 - val_binary_accuracy: 0.7348\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4655 - binary_accuracy: 0.7905 - val_loss: 0.5464 - val_binary_accuracy: 0.7261\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4645 - binary_accuracy: 0.7914 - val_loss: 0.5439 - val_binary_accuracy: 0.7304\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4604 - binary_accuracy: 0.7961 - val_loss: 0.5441 - val_binary_accuracy: 0.7261\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4571 - binary_accuracy: 0.7914 - val_loss: 0.5452 - val_binary_accuracy: 0.7217\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4543 - binary_accuracy: 0.8017 - val_loss: 0.5464 - val_binary_accuracy: 0.7130\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4475 - binary_accuracy: 0.8035 - val_loss: 0.5466 - val_binary_accuracy: 0.7130\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4525 - binary_accuracy: 0.8017 - val_loss: 0.5467 - val_binary_accuracy: 0.7217\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4450 - binary_accuracy: 0.8035 - val_loss: 0.5461 - val_binary_accuracy: 0.7174\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4444 - binary_accuracy: 0.7961 - val_loss: 0.5483 - val_binary_accuracy: 0.7087\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4518 - binary_accuracy: 0.7886 - val_loss: 0.5469 - val_binary_accuracy: 0.7043\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4441 - binary_accuracy: 0.7952 - val_loss: 0.5474 - val_binary_accuracy: 0.7087\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4401 - binary_accuracy: 0.8045 - val_loss: 0.5487 - val_binary_accuracy: 0.7130\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4409 - binary_accuracy: 0.8045 - val_loss: 0.5490 - val_binary_accuracy: 0.7087\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4436 - binary_accuracy: 0.7980 - val_loss: 0.5495 - val_binary_accuracy: 0.7043\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4385 - binary_accuracy: 0.8054 - val_loss: 0.5493 - val_binary_accuracy: 0.7043\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4390 - binary_accuracy: 0.8035 - val_loss: 0.5501 - val_binary_accuracy: 0.7000\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4362 - binary_accuracy: 0.8026 - val_loss: 0.5508 - val_binary_accuracy: 0.7000\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4314 - binary_accuracy: 0.8063 - val_loss: 0.5498 - val_binary_accuracy: 0.6913\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4321 - binary_accuracy: 0.8091 - val_loss: 0.5516 - val_binary_accuracy: 0.7043\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4308 - binary_accuracy: 0.8110 - val_loss: 0.5496 - val_binary_accuracy: 0.7000\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4320 - binary_accuracy: 0.8110 - val_loss: 0.5501 - val_binary_accuracy: 0.6870\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4344 - binary_accuracy: 0.8054 - val_loss: 0.5508 - val_binary_accuracy: 0.6870\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4327 - binary_accuracy: 0.8073 - val_loss: 0.5510 - val_binary_accuracy: 0.6870\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4281 - binary_accuracy: 0.8091 - val_loss: 0.5500 - val_binary_accuracy: 0.6913\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4310 - binary_accuracy: 0.8073 - val_loss: 0.5519 - val_binary_accuracy: 0.6913\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4336 - binary_accuracy: 0.8119 - val_loss: 0.5528 - val_binary_accuracy: 0.6913\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4367 - binary_accuracy: 0.8054 - val_loss: 0.5538 - val_binary_accuracy: 0.6870\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4298 - binary_accuracy: 0.8091 - val_loss: 0.5544 - val_binary_accuracy: 0.6870\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4274 - binary_accuracy: 0.8063 - val_loss: 0.5538 - val_binary_accuracy: 0.6913\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4280 - binary_accuracy: 0.8110 - val_loss: 0.5535 - val_binary_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4254 - binary_accuracy: 0.8138 - val_loss: 0.5538 - val_binary_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4199 - binary_accuracy: 0.8091 - val_loss: 0.5540 - val_binary_accuracy: 0.7000\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4203 - binary_accuracy: 0.8184 - val_loss: 0.5554 - val_binary_accuracy: 0.7000\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4227 - binary_accuracy: 0.8147 - val_loss: 0.5553 - val_binary_accuracy: 0.7000\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4281 - binary_accuracy: 0.8026 - val_loss: 0.5554 - val_binary_accuracy: 0.7000\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4287 - binary_accuracy: 0.8017 - val_loss: 0.5558 - val_binary_accuracy: 0.6957\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4201 - binary_accuracy: 0.8119 - val_loss: 0.5562 - val_binary_accuracy: 0.7000\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4241 - binary_accuracy: 0.8110 - val_loss: 0.5575 - val_binary_accuracy: 0.6957\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4216 - binary_accuracy: 0.8128 - val_loss: 0.5575 - val_binary_accuracy: 0.6957\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4215 - binary_accuracy: 0.8110 - val_loss: 0.5579 - val_binary_accuracy: 0.6957\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4187 - binary_accuracy: 0.8063 - val_loss: 0.5575 - val_binary_accuracy: 0.6957\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4219 - binary_accuracy: 0.8156 - val_loss: 0.5577 - val_binary_accuracy: 0.6957\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4259 - binary_accuracy: 0.8110 - val_loss: 0.5583 - val_binary_accuracy: 0.6957\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4208 - binary_accuracy: 0.8063 - val_loss: 0.5603 - val_binary_accuracy: 0.7000\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4181 - binary_accuracy: 0.8091 - val_loss: 0.5604 - val_binary_accuracy: 0.6957\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4252 - binary_accuracy: 0.8119 - val_loss: 0.5612 - val_binary_accuracy: 0.6957\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4231 - binary_accuracy: 0.8184 - val_loss: 0.5606 - val_binary_accuracy: 0.6957\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4190 - binary_accuracy: 0.8175 - val_loss: 0.5612 - val_binary_accuracy: 0.6957\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4099 - binary_accuracy: 0.8156 - val_loss: 0.5611 - val_binary_accuracy: 0.7000\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4149 - binary_accuracy: 0.8166 - val_loss: 0.5606 - val_binary_accuracy: 0.7000\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4150 - binary_accuracy: 0.8128 - val_loss: 0.5605 - val_binary_accuracy: 0.6913\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4186 - binary_accuracy: 0.8250 - val_loss: 0.5610 - val_binary_accuracy: 0.6913\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4131 - binary_accuracy: 0.8175 - val_loss: 0.5617 - val_binary_accuracy: 0.6913\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4138 - binary_accuracy: 0.8166 - val_loss: 0.5609 - val_binary_accuracy: 0.6913\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4227 - binary_accuracy: 0.8119 - val_loss: 0.5649 - val_binary_accuracy: 0.6826\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4140 - binary_accuracy: 0.8240 - val_loss: 0.5649 - val_binary_accuracy: 0.6826\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4175 - binary_accuracy: 0.8101 - val_loss: 0.5629 - val_binary_accuracy: 0.6913\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4099 - binary_accuracy: 0.8101 - val_loss: 0.5620 - val_binary_accuracy: 0.6957\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4120 - binary_accuracy: 0.8194 - val_loss: 0.5624 - val_binary_accuracy: 0.6957\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4134 - binary_accuracy: 0.8101 - val_loss: 0.5616 - val_binary_accuracy: 0.6913\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4171 - binary_accuracy: 0.8147 - val_loss: 0.5630 - val_binary_accuracy: 0.6870\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4129 - binary_accuracy: 0.8128 - val_loss: 0.5627 - val_binary_accuracy: 0.6913\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4166 - binary_accuracy: 0.8194 - val_loss: 0.5611 - val_binary_accuracy: 0.6913\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4089 - binary_accuracy: 0.8194 - val_loss: 0.5560 - val_binary_accuracy: 0.6957\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4170 - binary_accuracy: 0.8175 - val_loss: 0.5571 - val_binary_accuracy: 0.7043\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4124 - binary_accuracy: 0.8240 - val_loss: 0.5589 - val_binary_accuracy: 0.6870\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4050 - binary_accuracy: 0.8147 - val_loss: 0.5598 - val_binary_accuracy: 0.6913\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4149 - binary_accuracy: 0.8175 - val_loss: 0.5606 - val_binary_accuracy: 0.6870\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4121 - binary_accuracy: 0.8175 - val_loss: 0.5620 - val_binary_accuracy: 0.6870\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4159 - binary_accuracy: 0.8128 - val_loss: 0.5621 - val_binary_accuracy: 0.6870\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4070 - binary_accuracy: 0.8212 - val_loss: 0.5630 - val_binary_accuracy: 0.6870\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4082 - binary_accuracy: 0.8203 - val_loss: 0.5637 - val_binary_accuracy: 0.6870\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4080 - binary_accuracy: 0.8222 - val_loss: 0.5653 - val_binary_accuracy: 0.6826\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4051 - binary_accuracy: 0.8194 - val_loss: 0.5654 - val_binary_accuracy: 0.6826\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4058 - binary_accuracy: 0.8259 - val_loss: 0.5654 - val_binary_accuracy: 0.6826\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4118 - binary_accuracy: 0.8277 - val_loss: 0.5659 - val_binary_accuracy: 0.6826\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4093 - binary_accuracy: 0.8156 - val_loss: 0.5659 - val_binary_accuracy: 0.6870\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4075 - binary_accuracy: 0.8091 - val_loss: 0.5656 - val_binary_accuracy: 0.6870\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4135 - binary_accuracy: 0.8231 - val_loss: 0.5660 - val_binary_accuracy: 0.6913\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4071 - binary_accuracy: 0.8296 - val_loss: 0.5658 - val_binary_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4117 - binary_accuracy: 0.8175 - val_loss: 0.5666 - val_binary_accuracy: 0.6870\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4000 - binary_accuracy: 0.8250 - val_loss: 0.5668 - val_binary_accuracy: 0.6870\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4110 - binary_accuracy: 0.8194 - val_loss: 0.5665 - val_binary_accuracy: 0.6870\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4007 - binary_accuracy: 0.8175 - val_loss: 0.5664 - val_binary_accuracy: 0.6870\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4030 - binary_accuracy: 0.8240 - val_loss: 0.5661 - val_binary_accuracy: 0.6870\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4119 - binary_accuracy: 0.8194 - val_loss: 0.5661 - val_binary_accuracy: 0.6870\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4106 - binary_accuracy: 0.8250 - val_loss: 0.5663 - val_binary_accuracy: 0.6870\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4121 - binary_accuracy: 0.8194 - val_loss: 0.5663 - val_binary_accuracy: 0.6870\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4010 - binary_accuracy: 0.8268 - val_loss: 0.5681 - val_binary_accuracy: 0.6826\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4027 - binary_accuracy: 0.8268 - val_loss: 0.5672 - val_binary_accuracy: 0.6913\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4036 - binary_accuracy: 0.8156 - val_loss: 0.5683 - val_binary_accuracy: 0.6826\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4022 - binary_accuracy: 0.8222 - val_loss: 0.5699 - val_binary_accuracy: 0.6826\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3994 - binary_accuracy: 0.8296 - val_loss: 0.5706 - val_binary_accuracy: 0.6826\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4011 - binary_accuracy: 0.8315 - val_loss: 0.5704 - val_binary_accuracy: 0.6826\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4012 - binary_accuracy: 0.8287 - val_loss: 0.5703 - val_binary_accuracy: 0.6870\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4034 - binary_accuracy: 0.8287 - val_loss: 0.5703 - val_binary_accuracy: 0.6913\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4041 - binary_accuracy: 0.8296 - val_loss: 0.5712 - val_binary_accuracy: 0.6870\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3927 - binary_accuracy: 0.8399 - val_loss: 0.5726 - val_binary_accuracy: 0.6783\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3964 - binary_accuracy: 0.8287 - val_loss: 0.5712 - val_binary_accuracy: 0.6870\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3995 - binary_accuracy: 0.8296 - val_loss: 0.5699 - val_binary_accuracy: 0.6826\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3996 - binary_accuracy: 0.8287 - val_loss: 0.5713 - val_binary_accuracy: 0.6826\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3984 - binary_accuracy: 0.8333 - val_loss: 0.5731 - val_binary_accuracy: 0.6826\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3934 - binary_accuracy: 0.8287 - val_loss: 0.5730 - val_binary_accuracy: 0.6870\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3922 - binary_accuracy: 0.8371 - val_loss: 0.5727 - val_binary_accuracy: 0.6870\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3981 - binary_accuracy: 0.8287 - val_loss: 0.5745 - val_binary_accuracy: 0.6783\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3987 - binary_accuracy: 0.8268 - val_loss: 0.5746 - val_binary_accuracy: 0.6826\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3941 - binary_accuracy: 0.8259 - val_loss: 0.5747 - val_binary_accuracy: 0.6826\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3926 - binary_accuracy: 0.8240 - val_loss: 0.5762 - val_binary_accuracy: 0.6783\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3976 - binary_accuracy: 0.8324 - val_loss: 0.5740 - val_binary_accuracy: 0.6957\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3967 - binary_accuracy: 0.8324 - val_loss: 0.5747 - val_binary_accuracy: 0.6957\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3957 - binary_accuracy: 0.8175 - val_loss: 0.5764 - val_binary_accuracy: 0.6957\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3926 - binary_accuracy: 0.8324 - val_loss: 0.5765 - val_binary_accuracy: 0.6913\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3930 - binary_accuracy: 0.8352 - val_loss: 0.5771 - val_binary_accuracy: 0.6870\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3930 - binary_accuracy: 0.8352 - val_loss: 0.5767 - val_binary_accuracy: 0.6870\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3976 - binary_accuracy: 0.8287 - val_loss: 0.5746 - val_binary_accuracy: 0.7000\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3962 - binary_accuracy: 0.8343 - val_loss: 0.5757 - val_binary_accuracy: 0.6957\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3974 - binary_accuracy: 0.8371 - val_loss: 0.5763 - val_binary_accuracy: 0.7000\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3930 - binary_accuracy: 0.8324 - val_loss: 0.5774 - val_binary_accuracy: 0.7000\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3921 - binary_accuracy: 0.8324 - val_loss: 0.5776 - val_binary_accuracy: 0.7043\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3942 - binary_accuracy: 0.8380 - val_loss: 0.5806 - val_binary_accuracy: 0.6870\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3931 - binary_accuracy: 0.8343 - val_loss: 0.5804 - val_binary_accuracy: 0.6913\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3898 - binary_accuracy: 0.8380 - val_loss: 0.5774 - val_binary_accuracy: 0.7087\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3949 - binary_accuracy: 0.8333 - val_loss: 0.5777 - val_binary_accuracy: 0.7087\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3856 - binary_accuracy: 0.8417 - val_loss: 0.5783 - val_binary_accuracy: 0.7087\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3908 - binary_accuracy: 0.8371 - val_loss: 0.5771 - val_binary_accuracy: 0.7130\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3897 - binary_accuracy: 0.8250 - val_loss: 0.5796 - val_binary_accuracy: 0.7000\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3904 - binary_accuracy: 0.8352 - val_loss: 0.5807 - val_binary_accuracy: 0.7043\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3885 - binary_accuracy: 0.8352 - val_loss: 0.5799 - val_binary_accuracy: 0.7000\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3889 - binary_accuracy: 0.8389 - val_loss: 0.5792 - val_binary_accuracy: 0.7130\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3920 - binary_accuracy: 0.8352 - val_loss: 0.5780 - val_binary_accuracy: 0.7000\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3963 - binary_accuracy: 0.8277 - val_loss: 0.5777 - val_binary_accuracy: 0.7087\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3897 - binary_accuracy: 0.8333 - val_loss: 0.5776 - val_binary_accuracy: 0.7174\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3854 - binary_accuracy: 0.8333 - val_loss: 0.5798 - val_binary_accuracy: 0.7174\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3906 - binary_accuracy: 0.8305 - val_loss: 0.5835 - val_binary_accuracy: 0.7130\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3864 - binary_accuracy: 0.8352 - val_loss: 0.5852 - val_binary_accuracy: 0.7043\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3849 - binary_accuracy: 0.8399 - val_loss: 0.5868 - val_binary_accuracy: 0.7087\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3953 - binary_accuracy: 0.8305 - val_loss: 0.5856 - val_binary_accuracy: 0.7087\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3874 - binary_accuracy: 0.8399 - val_loss: 0.5875 - val_binary_accuracy: 0.7087\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3879 - binary_accuracy: 0.8333 - val_loss: 0.5858 - val_binary_accuracy: 0.7130\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3880 - binary_accuracy: 0.8473 - val_loss: 0.5869 - val_binary_accuracy: 0.7087\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3833 - binary_accuracy: 0.8380 - val_loss: 0.5861 - val_binary_accuracy: 0.7174\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3902 - binary_accuracy: 0.8333 - val_loss: 0.5845 - val_binary_accuracy: 0.7087\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3825 - binary_accuracy: 0.8408 - val_loss: 0.5851 - val_binary_accuracy: 0.7174\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3864 - binary_accuracy: 0.8417 - val_loss: 0.5860 - val_binary_accuracy: 0.7130\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3853 - binary_accuracy: 0.8380 - val_loss: 0.5878 - val_binary_accuracy: 0.7130\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3894 - binary_accuracy: 0.8343 - val_loss: 0.5880 - val_binary_accuracy: 0.7130\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3834 - binary_accuracy: 0.8426 - val_loss: 0.5873 - val_binary_accuracy: 0.7130\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3832 - binary_accuracy: 0.8361 - val_loss: 0.5883 - val_binary_accuracy: 0.7130\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3877 - binary_accuracy: 0.8352 - val_loss: 0.5885 - val_binary_accuracy: 0.7174\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3887 - binary_accuracy: 0.8333 - val_loss: 0.5880 - val_binary_accuracy: 0.7217\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3795 - binary_accuracy: 0.8501 - val_loss: 0.5893 - val_binary_accuracy: 0.7217\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3872 - binary_accuracy: 0.8380 - val_loss: 0.5905 - val_binary_accuracy: 0.7043\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3797 - binary_accuracy: 0.8408 - val_loss: 0.5913 - val_binary_accuracy: 0.7087\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8473 - val_loss: 0.5912 - val_binary_accuracy: 0.7130\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3790 - binary_accuracy: 0.8389 - val_loss: 0.5919 - val_binary_accuracy: 0.7130\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3754 - binary_accuracy: 0.8371 - val_loss: 0.5946 - val_binary_accuracy: 0.7087\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3790 - binary_accuracy: 0.8454 - val_loss: 0.5931 - val_binary_accuracy: 0.7087\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3880 - binary_accuracy: 0.8380 - val_loss: 0.5937 - val_binary_accuracy: 0.7130\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3859 - binary_accuracy: 0.8361 - val_loss: 0.5941 - val_binary_accuracy: 0.7130\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3860 - binary_accuracy: 0.8380 - val_loss: 0.5943 - val_binary_accuracy: 0.7130\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3864 - binary_accuracy: 0.8399 - val_loss: 0.5936 - val_binary_accuracy: 0.7130\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3858 - binary_accuracy: 0.8268 - val_loss: 0.5935 - val_binary_accuracy: 0.7043\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3871 - binary_accuracy: 0.8399 - val_loss: 0.5939 - val_binary_accuracy: 0.7043\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3786 - binary_accuracy: 0.8510 - val_loss: 0.5955 - val_binary_accuracy: 0.7174\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3851 - binary_accuracy: 0.8352 - val_loss: 0.5967 - val_binary_accuracy: 0.7087\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3818 - binary_accuracy: 0.8389 - val_loss: 0.5950 - val_binary_accuracy: 0.7087\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3835 - binary_accuracy: 0.8426 - val_loss: 0.5955 - val_binary_accuracy: 0.7087\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8436 - val_loss: 0.5939 - val_binary_accuracy: 0.6957\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3839 - binary_accuracy: 0.8445 - val_loss: 0.5945 - val_binary_accuracy: 0.7130\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3818 - binary_accuracy: 0.8436 - val_loss: 0.5952 - val_binary_accuracy: 0.7087\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3798 - binary_accuracy: 0.8436 - val_loss: 0.5958 - val_binary_accuracy: 0.7087\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3729 - binary_accuracy: 0.8445 - val_loss: 0.5981 - val_binary_accuracy: 0.7087\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3803 - binary_accuracy: 0.8389 - val_loss: 0.5981 - val_binary_accuracy: 0.7087\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3819 - binary_accuracy: 0.8436 - val_loss: 0.5972 - val_binary_accuracy: 0.7174\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3710 - binary_accuracy: 0.8445 - val_loss: 0.5983 - val_binary_accuracy: 0.7130\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3796 - binary_accuracy: 0.8324 - val_loss: 0.5970 - val_binary_accuracy: 0.7087\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3815 - binary_accuracy: 0.8408 - val_loss: 0.5961 - val_binary_accuracy: 0.7043\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3781 - binary_accuracy: 0.8436 - val_loss: 0.5979 - val_binary_accuracy: 0.7087\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3766 - binary_accuracy: 0.8361 - val_loss: 0.5980 - val_binary_accuracy: 0.7130\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3718 - binary_accuracy: 0.8399 - val_loss: 0.5992 - val_binary_accuracy: 0.7087\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3839 - binary_accuracy: 0.8343 - val_loss: 0.5964 - val_binary_accuracy: 0.7087\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3748 - binary_accuracy: 0.8510 - val_loss: 0.5965 - val_binary_accuracy: 0.7130\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3748 - binary_accuracy: 0.8436 - val_loss: 0.5976 - val_binary_accuracy: 0.7130\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3746 - binary_accuracy: 0.8352 - val_loss: 0.5994 - val_binary_accuracy: 0.7130\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3753 - binary_accuracy: 0.8492 - val_loss: 0.5999 - val_binary_accuracy: 0.7217\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3740 - binary_accuracy: 0.8482 - val_loss: 0.6018 - val_binary_accuracy: 0.7217\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3752 - binary_accuracy: 0.8538 - val_loss: 0.5994 - val_binary_accuracy: 0.7304\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3756 - binary_accuracy: 0.8305 - val_loss: 0.5984 - val_binary_accuracy: 0.7130\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "             metrics=['binary_accuracy'])\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model.hdf5',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_data_x, \n",
    "          train_data_y, \n",
    "          validation_data=(validation_data_x, validation_data_y),\n",
    "          batch_size=8, \n",
    "          epochs=200,\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fabf388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAAsTAAALEwEAmpwYAAA/1UlEQVR4nO2dd5gUVfb+3zszMOQkQ06DBAFRksAiMqhfBFkFAwiKCsKiKBhXBRPrD8WIuyZUMi4GRNQVFVFBQCWNgEgOI3nIGckzfX5/vH2t7p7UM9NhqDmf5+mnum5V3bpd3f3WqXPPPdeICBRFURT3EhPtBiiKoijhRYVeURTF5ajQK4qiuBwVekVRFJejQq8oiuJy4qLdgEAqVqwoderUiXYzFEVRziuWLVt2QEQSMttW4IS+Tp06WLp0abSboSiKcl5hjNmW1TZ13SiKorgcFXpFURSXo0KvKIriclToFUVRXI4KvaIoistRoVcURXE5KvSKoiguR4VeURQlmpw9C4wbB5w7F7ZTBCX0xpguxpgNxpgUY8ywTLbXMsbMNcb8ZoxZaYzp6i2vY4w5ZYxZ4X29F+oPoCiKcl7z0UfA3XcD8+eH7RQ5jow1xsQCGA2gE4CdAH41xswQkbU+uz0NYJqIvGuMaQxgJoA63m1/iEizkLZaURQlP3g8wLFjQLlykTnfn38CRYvyFchnn3G5b1/YTh+MRd8aQIqIbBaRswCmAugesI8AKON9XxbArtA1UVGUQk96OvD990CoZsT78EOgZk2KfSRo2xYYOjRj+dGj/FwAcOBA2E4fjNBXB7DDZ32nt8yXZwHcbozZCVrz9/tsS/S6dOYbY67I7ATGmLuNMUuNMUv3798ffOsVRSkcfPUV0LkzEKo8WIsW0crevDk09WXHuXPA2rXAN984ZUePAk8/Dbz2Gn30QNSFPhhuBTBZRGoA6ApgijEmBsBuALVEpDmARwB8ZIwpE3iwiIwVkVYi0iohIdPka4qiFGa2bOFy7drs9wsWW8+OHdnvFwr27OGTyKZNwO7dLJsxAxg5EnjuOaBaNaBCBSCMRm4wQp8KoKbPeg1vmS8DAEwDABFZBKAYgIoickZEDnrLlwH4A0CD/DZaUZTziIMHgQsuAObMyXsdO3dyuWGDU9avH9CjR97qs0K/fXve2xQsqT5yaTtcN28GjAH+3/8D3noLqFQp6hb9rwDqG2MSjTFFAfQGMCNgn+0ArgYAY0wjUOj3G2MSvJ25MMbUBVAfQASelRRFKTCsXg0cOgR8913e6wgU+jNngOnTgZkzgbQ0lokAkyfn7Hffv9+xniNh0du2A47Qb9lCS374cOCmm4CKFaMr9CKSBmAIgO8ArAOja9YYY0YYY7p5d/sngIHGmN8BfAygn4gIgA4AVhpjVgCYDmCQiBwKw+dQFKWgYv3gy5fnvQ5rFW/cyOWCBcCJE8CpU8CqVSxbuhS46y5g7Njs61q3znmfmUW/axfw2GPAk08yEkaEN6k773TacewY8MILwCuv5Nx2K/Rt2gA//cT3mzcDdes6+4RZ6IOaeEREZoKdrL5lw33erwVweSbHfQbgs3y2UVGU8xnrX1++nKJpTNb7ejzsqCxf3r/ciuWmTYzAmTWL9YgAixcDzZtzCdBqfvTRrM9h3TZ16/pb9Hv3Ai+9BLz7Ls/h8QCvvgrEx/OmAgCJicDgwcCll9L3Hh8P/POfQGwscPgw0KkTo2t69vRve/HiwI03AsOG8WliyxbgqqucfSpWdNofBnRkrKIUdnbtyp+1nRPWoj98GNi61SlPSwO+/RY4csQpmzwZqFHDP6bc46ElXakSXTbbt1Por7ySZVYg7fLnnynUWbF2LVCqFNCunWPRnzsHtG9Pf3mfPnxyWLuWlv3dd3Pkavv2dBdNmkSR/8c/nPYAFPxly4CPP/Y/386d/EyXe23h+fP5eXwt+oQEWvShCh8NQIVeUQo7jz0GdOgQvpjyLVscC93eUFasAC6+GOjaFRgyxNl39mzg5Eknthyg6KelORbwnDl013Tpwvj0JUtYvmQJUKIEnwhWrsy6PWvXAo0bA7VqUXDT04EPPgBSUijkEybQcm/YkO6Zf/+bot67N48dNQq44gq6cgD2G8yezRtAqVJ0z3g8zvms0LdoAcTFAZ98QkEPdN2kpYXtO1ChV5Rws2YNcP31FLCCyMKFdE1MnZr5dpHgLM3t24FrrmEI4aFDFOKUFFr0XbtS5JYt474vvkgBv+46WsCbNrHcWuWzZjn1WreNFfrHH2ddN9xAod+wgRb4H39QkAH/dAL79/P6z57NdV+hT09n/S+8QPdP98CxoD7ceCPdRQcO8DwNG7J8wwY+iVSqxJvAwYPsgO7TB/j8c0foS5Sgy+err3hcYqJTd8WKTlvDgAq9ooSbzz8Hvv46vO6RYNi/ny4HgEK8cyfXrTtl3LiMx5w8SctzxIic63/7beCHH9hxOXcul2PH8hyNGgFNmjh++vnzgWuvBcaPZ1qAF16g8G/ZAhQpwmOtVWyFvkULoGxZuoCefBKoXx/429+47cEHuezZkwJqhf7AAeDqq3n9v/wSOH6cN6IGDSj0ADtUU1IYAZNd/0G1anS/lC3LsM6EBL7fuJE3qPbteaMDgIcfZg6bV17hU0ONGixv25buHiCjRW/bGwZU6BUl3Fg3QnaDfTweYOJE4PTp0Jzz2DGKrA09BChOV1zBshtuoI96wQJu69WLUSsrVvjXM2YMbwSvvpq9CJ09S6sWoNVub2qTJnGZmAhcdhlHpK5cyY7PpCSgcmVg0CBgyhTHtz1gAM9l67BCX7Mmre6mTYGnnmLZFVfQCp81i1Z+ixYU9m+/pRumUyc+LVSqRDH/4w8eV78+6wPY+dq0KdDNBhFmw4QJPFeJErwpNGwI/PIL623bFqhTh/X++CP3X7KE19sKfZs2XMbHA1WqOPWGWeghIgXq1bJlS1EUV9GgAZ0fDz6Y9T4//MB9Jk0KzTn79WN9H3/M9Z07rQNG5K67nPfNmokUKSKyYwfXX3jBqePkSZEqVUQaNxYxRuSpp7I+3/TpPL5UKZF27UQ6d3bOAYgsWiQyZw7fd+zI5fr1PDY1VSQ+XqRYMZG4OJGtW3m+228XOXpUZOhQkaJFRdLTRQ4dYpkvZ86I3HqryC23cH3/fpGmTXmOokVFZs3itnr1RD79lOW//cZ6bPumTcvbdb7jDqeOn35iWZ8+XH/6aWfbl19y28aNXL/oIv96/vgj398/gKWSha5GXdgDXyr0iqs4cUIkJoZ/tU6dst7vuee4zz335K7+adNEEhJEypQR+fe/Wfbtt6zLGJGrr2bZm2+yrGZNLqtWFalcme8vu4z7VK3KG0RamkiLFiIlS3L73LkiPXrwHH/+mXk7OncWqVFDZMgQkRIlRCpWFGnSxBG6PXso1HXrcr1yZRGPxzl+8GCW2///oEFcT0gQadVKJDExd9dl716RXr0o8iIiTzzBm4i9zseOsbxsWd7I0tNzV7/F1hcby+9ahILfr5/IuXPODWfZMm7zeEQuuECka1f/euxN59VX89YOyV7o1XWjKOFk7Vq6ZcqVy+i68XjoYvB4MoYIBsOuXQz9q1qVLpCJEymrDz8MXHQR/dhz5tCtMH06o1xef53HDh3KFAIAXQ4A3RAbNtDFsXw50LEj909KAvr3pzto0SL67adMoRsjNRXYto1RMv370z1z8iRdEAMHModLiRJ0ncTEOJ2lSUn+/vChQ+mrtyGI774LJCfz2KVLHddHsFSqxM7lzp25Xq8eXSg//shrVbo0y8eP52eJyaMU2g7ZSy9lWwG6kyZNoivpllsYY2/7A4wB3n+fqQ98KV2an19dN4pyHjJxIi21/v25PHLE2TZlCss++IBWXkwMX8ePZ6xn+XKRgQNp8a9bx7KbbqK7Y+NGkVdeYV3ffMPlmDF0x8TE0JVijMizz9Ki/PlnWu2bNokULy7y9des7+67RSpU4FMCwHNajh5lXU8/TavTWuoXXijy6KOsf+tWkVWrnG0LFrDOpCSnnl27REqXFvnvfzN+xpUr6ZrxZfNmkdq1Re67Ly9X32H+fLapSBGRyy/PX12+/P4767333sy3nz4t8uuvwdVVrZrIgAF5bgrUdaMoEWDKFLo4Tp50yh56iK6Mzz+Xv3zVlg4dHLEERG680XGVBNKnD10PxtBnfeYM3z/yCLevXMlj69alIO/dy/LBgykg9evTDxzIqVPO+9dec0QrNtZ/mwhdKFdcIdKmjUjz5iIzZjii3rkz9zl3jjePmBi6edLSWBZ4Tl+3TU6cPp1314olNdVpa9+++avLl9OnRa66ijeS/HLJJSLduuX58OyEXl03SuFkxw6G2oWK//6XA2imTweefdYpX7mSLpOmTbn+++8sW7eOA2sSE51IkPu90zhk5r5ZvJhRIXXr0lWycydl6+KLuf3ii4Hq1RmznpRE1wXAkMfUVIYA+obzWYoVc95bN8TnnzP80HcbwHoXLWIkSc+ejE0fNIjbrEsmLo6RMY0aASVL0m0RF5BppVix7MMYA4mPz7trxVK1KtMQAHTjhIr4eLrHOnTIf11hzHcTVK4bRXEV6en0JffsySHv+eXIEQrdlVcytG7UKArcunX0CT/wAAU9Ph647z765EuVogB+/jnQqhXFr0MHhv0tXOhf//79vBnccw9HfW7d6sS+167NpTEcoDRhQt5T91qh37uX/vlAkpI4UQYA3Hwzl6+9Rp/0TTc5+40dG9aJrvOEMRT4VatCK/Sh5N57w3bd1KJX3MuJE0xAFTgr0a+/UswCY8bzyoIF/IM+8wzwxhscdfnii8C8ebTuR46k8P/97xT1UaMo0HfdBTRrxuXNN3Ofa6/lyMkJE5z67RD/tm153LZtfAGO0AMciVmzpiPCuaVOHQ5WAoBLLsm4/YorKJiXXEKLH2AH5G23+VvcTZrwcxU0rMAXVKHv0QO49dawVK0WveJOTp7k8Pp58xjN0KqVs80Or/edxCIYDh0CWrfm43VSEkdaAhyFWbQoB8MUL07XjHizNPq6KOwk0AATYFl8R6S+/DLbNXAgBx499RTdNrGxQMuW/Dy7d3MfY5xBPwCfKPIzkUZcHHDhhcD69ZkLfblyTD/gey3PJ6zAX3hhdNsRBVToldCxeTPdIvXrR7slHLI/fz6HqAcKuhX6/fs5nD41lfv5iqZFhMP6r7rKCVVs0oRD6s+coTtm/nzeAKwPOFDgc0OxYsAXX/BGMG4cE2BVruyE71kL/uefOSS/aNG8nScrGjbMWugBpvE9Xxk8mE9bgSmQCwHqulFCx8CBmbsNli7NmLo1WA4f5ryaufVdzpjB4e9duvgL/cGDjM9u3pzrGzfSXXLXXc4+f/5Jl8vx4xT5zp3Z2Tp/PjsYhw2jn33DBu6zbBkt/FBRvDjwzjtMjFWiBH39Nta9Th0uk5Od96GkXTv2J2R20zvfqV3bGTtQyFChV0LH6tXs7AqMHHjpJXYkSh5ybU+ezGRT8+YFt7/HQ//1unUU+YYN2XFpE0nNns122CRY33zDCJa5c53MgdOmcUDL+PHAp5+y7NNPKfTt2tG6BjgAauFCPsWEUugtDRuyXUlJ9L8DjkWflubvnw8Vjz5Kiz6vTyRKgUSFXgkNR444k0XY6dIs1vI9lIdZJK2bJavMj0eO0C999ixHITZrxkgWgJZ4gwYUfxvCuHo1Ow579qTf23Z6ejzA//7nf86xY+lGiY3lDWL1aopugwasY+1a3oBiY50siqGmXj2eo107rlevzvMB4bHoY2JC7w5Soo4KvRIa7FyegH8ucI/HyTVuZxoKlpMnnbqyEvp77qHgXXopre5Vq+haqVmTsdy+OcNtG2rVokukbl2mEahUiYI6fTot5R9+YEzz+vV09Tz2mJMFMimJfvkLL6TQf/cdRbhUqdx9trwSF+ekAwiHRa+4EhV6JTRYIa1Z01/ot2933CZ27lARivFvv2Ws59lnOUlF374U3jNnKMR2wgpfRGjtJiYyXPLNN9nhdvYs3TbGOGGAtn1btjgDh+xNICmJoW1z5vCcR44w+qVUKfrkn3mGn6tYMcbfA+zUW7CAn6FLlzxetDxiBV6FXgkSFXq3cv/9/iF8oWDRIlrJr76acbakjRvpUrjzToYXHj7Mct+OUGvRb9tGIR0/3r+OgwcZLbNuHaNN7rqLHZODBtH1cuQII2TWr2eM/LZtdBc9+ijdQvffz/6Anj2Z7AsAypRh3m/7xLF5szOzj70JJCXxXCVK0BceE8N87S+8APzrXywfMQJ44gla8wCF3k7iEWmhty6bcLhuFFeiQu9GRDgH5uuv+7tU8sv771O4H3+cg2cOH+bgoz/+YHliIiNYRDg4KDnZEfr4eMeit26YQCv9hx947NSpjJopUoSTSFj/9JNPOi6ZTp2cVAE2IgWgFT5tmn+st83KeOIELX9r0du0BFdeSdH/9lveWNq1Y9bF+++n2wZgtMbw4U6djRtzWalS5AcHNWzI62kzIipKDmgcvRvZsoXWL0Cr1M78kxMiTAnQpYtj7fpumzWLlm6/frSa69enFV6vHoWnYUOmmR03jqJ43XXcv0wZps21Fr0V+N9/p+/b5kKZNYsC26oVnw5Wr2Z8u43WefddDhpq04bhh/HxFGYr2FnRsCEHK9m0Adai79OHNw0r2pdfzvld7ejQ7LDHdO6c/zwsueWBB3gjtXH7ipITWWU7i9ZLs1fmA5sR0M6ik5TELIRbtwZ3vJ2F6OabM25bt47b3nuP6199xUkV+vZ1sgI+/LCz/8yZLCtenBNb9O7NLI0iIl26OMesXMnUrOPGcTKKXr0yb1uNGkwxu3q1yMGDnJEIYAbInHj7be77n/9wuXhxcNcjO06d4rnnzct/XYoSAqDZKwsBZ87QEh4yhK6RuDhOTJye7uRKCeT4cf8h89al8uWXdHH4YkMO7UQO111HX/zEiU4GRdu5CXCS5Fq1gFOnWG6zLqal0aK37pbXX2eI48CBPGdW/u7nn+dkDk2a0Oq3ibt83TZZYdv83ntcWos+PxQrxk7ncMTPK0qIUaE/n9i1y4kRD+T55ynUEyYwY+LFFzsC7NshumYN69i1i8P2L7rIiZKxLpW0NPrjfZk1i/sGdgDGxLDDEvD3VcfGcsYhgG6gxETWu2QJBybdeisjWiZOpGuna1eKpxXlQPr2dQYNAQyrBIIT2nr1GA65YQPPmZCQ8zGK4iayMvWj9Sq0rpuJEzmP5A03cOafzLjtNroeduzwL1+5kpNStGvnuET69+e2WrU4aYXFTtocF8cJMerX53L5cpHrrxdp1EikfXuWW1fQokWc5GLYsKzbn9mkFjt2iFSvzkkZ7MTQDz3E5cKFnOnHTnSRliaye3fw10uEsw8FO4GFnZO0adPcnUNRzhOgrpsCzltv0frduJHJst5919nm8TijSm32w++/59KGOL7+OjsmZ8xwcri0bMllw4b+kTe//05LvlMnDv//6Sce+/LLfCJo0YJulE2buO3MGbatRg2GF2ZFZpNa1KjB9AIdOjjb33qLTwGXXMJzATxfbCzDIHNDYmLwQ/WtSygUbhtFOc9QoY82s2YxiuLGGznSsnNnCroIcPo0wxVr1GCs+JkzdD3MmsW855UqMdfK1KlA797ABRc48eOtW3PZoAFdFiKMOd+zB+jVC5g5k5NLVKkC3HEHz5maSvHt0YPRLuPHM6HYunVMB1CmTN4/Z+3ajHG/7z76ykuWZL6ZMWOcm1M46diRUSq+/QiKUljIytSP1qtQuW6OHmU0SePGnHtSRGTSJLoYfvmF0SnGiFSpwrJWreiSKVtWpGJFlpUs6R9JkpYm8tNPzjnefJPbd+8WmT2b73/4wb8dvhM62yiS++4TKVqUUTuhnGMzmmQ2+bSiuASo66aAIAHZG598kp2iEyc6Iy67dWPETNeutNzHjWMHZocO7PTs0oXTyR04wJGvJ04wjtxa8LGxHMxk8c31smoV3wfmGr/4Yid6xXao/uMfTCWQkAD8+98huwRRpWnTQpmLXFF0wFS42LqVA4QuuIAJt5KTKeLjxnFS5W3b6A65+24OALJUqMDRoN99RxfHgAEst5Exhw9TzK+6iqkIypfn8Vn5qu3Ap40bGQ5ZqZIzcbQvo0bRJ1+2LNebN+egp6uuYpsURTlvMRJoZUaZVq1aydLAOT4LEh4P8OGH7MysXJmhilde6S+GIkwnu3s31zdsYNjjK68wBezkyRTVCROYPiBwkodt23iTuPLKzNswaxbjyYOZHMLjoT988GAmACtfnqkGFEVxFcaYZSKS6TyPQblujDFdjDEbjDEpxphhmWyvZYyZa4z5zRiz0hjT1WfbE97jNhhjsgiSPk/weGiB33knXSn33suOy+ef999v+3aK/MCBXJ85kxZ5s2a07m+7jdb6XXdlLta1a2ct8gDdN8HOABQTw1QF8+czhj6rKeIURXEtOQq9MSYWwGgA1wJoDOBWY0zjgN2eBjBNRJoD6A3gHe+xjb3rTQB0AfCOt77zk1dfpRXety+FfMwYDvIJnP3IjjD9xz84yOizzzid3rXXMrXthAnMVfL005Fp94ABbNPp0znnhVEUxXUEY9G3BpAiIptF5CyAqQC6B+wjAGzsXVkAu7zvuwOYKiJnRGQLgBRvfecns2YxzcCkSZzibfRoZnJcscJJIgZQVGNjKapdugC//OJMN1ekCOPSv/46cvNyPvggQyRHjnRSByiKUmgIRuirA9jhs77TW+bLswBuN8bsBDATwP25OBbGmLuNMUuNMUv323k7CxoijFpp0YIdn61aMSa8Y0duW7DA2XfZMmY3LF7cGagTG+uk240GDRowyidSMyEpilJgCFV45a0AJotIDQBdAUwxxgRdt4iMFZFWItIqoaDmIdm9myl5A10fbduyg9VGxYhQ6O2ozw4d6N5p2RIoXTqybVYURUFw4ZWpAHx9DDW8Zb4MAH3wEJFFxphiACoGeWzBYuNG/1zsR47Qt71yJdcDOzOLF2cMuxX63bs5AtWmIChenHHoOkmEoihRIhir+1cA9Y0xicaYomDn6oyAfbYDuBoAjDGNABQDsN+7X29jTLwxJhFAfQDJoWp8yJk3jwOM5sxxym69lS6XFSu4nllnZlISrfjjx50MkNaiBxid8/e/h6vViqIo2ZKj0ItIGoAhAL4DsA6MrlljjBlhjOnm3e2fAAYaY34H8DGAft5RuWsATAOwFsAsAINFJD0cHyRXbN3K/DJ20mqLjZ6ZOpXLzZvZAbtlC2c0qlEj85GVHTqws3XhQvrq4+IYRqkoilIACGpkrIjMBDtZfcuG+7xfC+DyLI4dCWBkPtoYeqZOZRbFq68GuvsEENkJOr74ghkkJ05kHHqZMsCOHUxLkBnt2rGzdf58jmi9/HLt9FQUpcBQOHPdWH/79OlOmcdDoa9Rg52us2YxjLJLFycNQVaDjUqVYhTO9Ol08WQ1S5KiKEoUKNxCP2OG477ZtIl5ZIYNY8qAbt2YcGzIEI6GjY8H2rfPus6kJNYBqNArilKgKHxCf+YMc880bw4cO+Z0vC5ezOWVV3KA0VVXcUq+a69lFM6BA1m7bgBnSrsqVdQ/ryhKgaLwCf369Zy79OGHmanRztq0eDF98RddxBGks2f755spVSr72Yzat6efvnPn4Gc9UhRFiQCFL02xddu0akUXy7ffcpDT/PlM9xuTx3tfmTJMa2An5FYURSkgFD6LftUq+tvr16fQ794NTJvGXDDduuV8fHZ06cLOXEVRlAJE4RP6lSuZhyYuDrjmGpY98giXN90UvXYpiqKEicIl9B4PM0vaMMlq1fh+1y7GvlerFt32KYqihIHCJfQLFwL79zOSxmJDITV9r6IoLqVwCf306cwk6Rsm2acPQy179YpeuxRFUcJI4Ym68Xgo9F26+KcLvuQSZ0YoRVEUF1J4LPolS4DUVODmm6PdEkVRlIhSeIR+9mwOZLruumi3RFEUJaIUHqFfuRK48EKgXLlot0RRFCWiFB6hX7Uq80lDFEVRXE7hEPqTJ5lZMqs0w4qiKC6mcAj92rWMulGhVxSlEOLu8MozZzhRd1YTeyuKohQC3G3Rv/EGUK8e8MknQIkSQN260W6RoihKxHG30K9eDZw9C3z/PdMH5zUFsaIoynmMu5Vv82bmiQc04kZRlEKLu330W7Yw9XCrVkDHjtFujaIoSlRwr9CfOsX0w3XrAoMHR7s1iqIoUcO9rptt27hMTIxuOxRFUaKMe4V+82YuNdJGUZRCjvuFXi16RVEKOe4V+i1bOMlIlSrRbomiKEpUca/Qb95Ma96YaLdEURQlqrhX6LdsUf+8oigK3Cr0IrToVegVRVFcKvTHjgHHjwO1akW7JYqiKFHHnUK/Zw+XVatGtx2KoigFAHcLvUbcKIqiBCf0xpguxpgNxpgUY8ywTLb/xxizwvvaaIw54rMt3WfbjBC2PWtU6BVFUf4ix1w3xphYAKMBdAKwE8CvxpgZIrLW7iMiD/vsfz+A5j5VnBKRZiFrcTCo0CuKovxFMBZ9awApIrJZRM4CmAqgezb73wrg41A0Ls/s2QMUKQKULx/VZiiKohQEghH66gB2+Kzv9JZlwBhTG0AigB99iosZY5YaYxYbY27I4ri7vfss3b9/f3Atz449e4DKlXWiEUVRFIS+M7Y3gOkiku5TVltEWgG4DcDrxpgLAw8SkbEi0kpEWiUkJOS/FXv2qNtGURTFSzBCnwqgps96DW9ZZvRGgNtGRFK9y80A5sHffx8eVOgVRVH+Ihih/xVAfWNMojGmKCjmGaJnjDEXASgPYJFPWXljTLz3fUUAlwNYG3hsyNm7V4VeURTFS45RNyKSZowZAuA7ALEAJorIGmPMCABLRcSKfm8AU0VEfA5vBGCMMcYD3lRe8o3WCQvp6cC+fUCVKjh1CkhNBerVczZv3MjsCBUrcoZBRVEUtxPUVIIiMhPAzICy4QHrz2Zy3EIAkZ2V++BBin2VKnj1VWDkSAp79erAn38CbdoAR45w15QU4MIMPQaKoijuwn1hKT4x9D//DJw9C4wdy6IPPqDIP/881xcvjkoLFUVRIoprhd5TqQqWLmXRmDHAmTPA228DzZsDQ4cCJUsCyclRbKeiKEqEcK3Qp5ypiSNHgB492Dd79dXAmjXAkCFAXBzQsqUKvaIohQPXCn3y1koAgGeeAa6/nv2zSUlA797crXVr4Lff6NpRFEVxM0F1xp5X7NsHFC+O5JXFULIk0KQJMCOTVGqXXUZ3zqpVtO4VRVHcivss+rNngWLFkJzM8MnY2Mx3a92ayyVLAI8ncs1TFEWJNO4T+vR0HI8pi99+o9WeFbVrA5UqAYMHA2XLAps2AcuXc5zVxo2Ra66iKEq4cZ/rJi0NH5zrhbNngZtvzno3Y4ApU4BFixhr//bbwIED7Lj94QegQYPINVlRFCWcuE7o5Vwa3j7ZHy1bcnBUdlxzDV+bNgGTJgGnT7M8OZmWvqIoihtwnetmfmo9rE1rgCFDaLUHw5AhnEv83Dng4osp9CLAd98BaWnhba+iKIUbEWD2bC7DheuE/sttzVDcnEKvXsEf06YN0L490L070KsXsH49MHky0KULB1spiqKEi+++Azp1AubPD985XCf0B06XRJW4AyhePPhjjAHmzAE+/dSJxnnsMS7ffju8d1pFUQo3i7z5frduDd85XCf0h06XRIXYY7k+rmhRzj5oM1oePAi0aEHr/scfsz9WURQlr9gR+qlZzfIRAtwn9GdLokKR3Au9pUIFoH59ID4e+PJL4IILgNGjQ9hARVEULyKREXrXRd0cOlsKtYtuz1cdQ4cypXGNGsDAgcArrwDbtwO1aoWokYqiKGAK9UOH+F4t+lxw6GwplC/yZ77qGDAAePBBvh80iMv33stnwxRFUQKw1nzVqir0QePxAIfOlUaFovkTel9q12ZStHHjnDh7gI9cZ86E7DSKomTB6dPAjh3A4cPRbklo2b8fmDcPKF6cUTcq9EFy/DjgQSwqxIdO6AEOnjpwAPj6a6fshRfoyz91KqSnUhTFBxGGP9eqBSQkAFu2RLtFoWH+fKZgGTuWSRVr1+ao/HPnwnM+Vwm9veNXiD8Z0no7duRdd+FCp+yHH2hlfPJJSE+lKIoPc+YAK1cCN9zAGULXr492i0LDjz8CMTEU+gkTONWpiDNBXqhxldDbTo0K8SdCWm+RIgy1tP609HT8NXuVxtkrSvh4+21a8i+/zPVwujciSXIyR+EPHMi8WtWrszxcn8+dQl8stBY9wIFUy5fz0WrdOuDECaBdO2DZMqY6VpTCwpEjwKhRdF/+/nvwx6WkOGNStm8Hvvkm+/23bQO++opiWKcOy0IlhIcPA599lrdjRYAPPwSO5TGK24ZU2sGZgCP0u3blrc6cUKEPkssuoz9+zRrHsn/zTQ60yusPRlHOR95/nyPHn3oKuOOO4J9on3ySaUbS04HnnmOQQ3Y+dxvpNmgQ/2eVKoVO6MeN4zSjf/yR+2NXrABuv503urxgQyozE3q16IPgL6EvHvoeUvulJCfzVbYsJxpv3hz49deQn05RCizJyUC1aswDtWoV8MsvwR23ZAnHp6xbx/ciWYctnz5NMb7hBqBmTZZVrx46Idy0icu8zBttn+DHj/ePxAsWe05foa9YkS5iFfogsEJfvkTo4x7r1uWoWSv0l13GzpTLLqO/Pj095KdUlAJJcjIjYW6/HShfnn70nNizh+4aAJg7l0/GMTEUy8wi1z75hGlIfNOFh1LoU1K4zIvQJyez7QcP5i0YIzmZwR1NmjhlMTG8earQB8GhQ0AJcxLF4kPfO2oM78AffcRHN3s3bt2a/vp165x9RTjoavLkkDdDUcLCyZN0pSxenHHbgAH0lQP8j6Wk8HdfogTQvz9dl1lFi7z1FvDQQ/5Pve+9xzEvDz/M+i66KOPrgQeARo2AK690jgsU+uef576XXsobh2XChIz1NWnCLJGWnIT++eeBl17KfFtyMtC5M+vN7Cb37bd8EvENlbzzTqct48YxuCMuIC9BKG9kgbgqBcKhQ0CFmCMZr2CIeOwxumyKFAH69WOZr0vn4ov5fskSYOJEYNYsoE8f7q8oBZmPPuI4kUsvBdq2dcqPHeNv+dw53gisYNvffa9ewGuvAT//DPTs6V/niRPAM88AR4+ykzE2lsfZbI2PP86/amZZG5s1A/7xD/85JapX53iWM2foAho5EmjYkG6YUaM4edDZs8DTTwOlSjE+3TJzJvDxxxToU6eAnTv5v/ztN362wP/olCl8Ahk4kPmuLMePA2vX8rP+/e+cy8K3Y1WE51++nLmyevTg9ilTgA4dOAK2WTMKfyDXXccbblgQkQL1atmypeSV7t1FLolbIzJgQJ7ryC3p6SJly4rcc49T1qePiDEigMinn0asKYqSJzwekUsv5e+1Vy//bcuXs/xvf+P6iBH8bR85wvUzZ0SKFhV59NGM9Y4Zw2Ptf+HSS0WeeILv69TJfTsnTOCxW7aIvPIK369aJXLvvSLx8SL794tMncryb77xP/b660UaNeL71au5T7duXP72W8brUbIkt73yiv+2uXNZPnOmyLFjIqVLi9xxh7N90SLnMyclseyOO7jfsWO5/8y5AcBSyUJXXee6qRBzOGwWfWZYP719BNy7F5g2DbjvPoaEBeO/VJRIc+KE09/04YcMkyxSxHFpWOy6jU5JTqYVXbYs14sWZUCC/f2vXu3U+9ZbtF779OG21q39XZ65xUam7NgBvPMOkJTEp+jBg2nlv/wy8Prr7E/r0sX/2NatOdjq6FHnM912G5effea0+cQJPsWc8A7FeecdZ1tyMq10gP/50qVpmX/yCdMZAPy/lynDJ5n58/kU8cknQN++3D9qZHUHiNYrPxZ9kyYiNxX9SmTw4DzXkReeekokNlZk716R557jHX39esfqWLkyos1RlBzp25e/TfsqX16kXz8+nXo8zn4vvujsc/SoSKVKInfe6V/XkCG0gL/6yr9OQGT8eJElS/h+8mSRXbtE4uJE3nwz921etYr13HEHl9OmOduuuso552uvZTx21ixumzNHZNQovj94UKRKFf/23nmnyJo1fH/zzRk/DyDSoIFT79q1LHvhBZE9e0SKFBF54AGRAwdEihd3rPu1a3P/eXMLsrHo3eejx8GIWvQALZaRIxluNmYMJxxv2JAdVcOH0yp4992INklRsmXjRlrizz/P9Xr16MeePJn/I+uX9rXwf/wR2LcvozXeujUt2YceotU9dizL4+PZmRoTwzDMRo3op1+5kufLLdai/+ADRqjccIOzbepU9h8UKcKUJYFcdhmXycn0vVeowNdPPzmhliNGsJ22Q/SBB+iDD/SbN2rk//7qq/n/PnuW/v777uP1W7qU/Q+VKvkfExWyugNE65VXi97joZ/u8SL/FvnnP/NUR364+mrezQGRL790yvv3FylRQuTw4Yg3SVGypFYtf9+yiMiMGfz9Ll7slCUliZQrx/KePblMTvY/bv16x9p97rnwtdnjcazkESNyf3y9eiI33ijSqZNI69YZt99/P33pkybxHCkpwdX7xRfcv0gRkWuuyX27QgUKg4/+1Cn66SrIQZoNEWbwYN7Na9dmb7xv+cmT9Ac+/XTG1MbnztG3ePRoZNt7PvDll3mLc1b8OXGCvzE7uMfjYRSMtZAt1sretIkW+qZNtOj/7/9Y/tVX9Mlfcon/cfXrO9FoAweG73MYwzbn9TytWzMt8NKlmT9R1KvHqJoVK7herVpw9V53HQd1nTvnH/dfkHCN6+bYMSY/Sji4F4irFPHzX389cNVVHETie59p0YIhVnPmML62bl26dCy//AIMG8Yb1bPPRrzZBRYR4K67gCuucDrAlLzx7rv8jTVtCnTtyo7DtLSMQpaYSDH98EOGBl9zDd0Yl14KLFgA7N5NsYyP9z8uJobhxvHxQOXK4f0s3brRM1ulSu6PvflmZp0VydhZCzjiP38+3TrFiwdXb1wc8K9/sePV18grUGRl6vu+AHQBsAFACoBhmWz/D4AV3tdGAEd8tvUFsMn76pvTufLTGSseD5+hhg/Pex1hwuMRufhikWbN/Du7xo5lk6tUYaiaQjZt4nVp0SLaLTm/SUsTSUzktRw7lmU2ZPKzzzLuX6tWxs7Hjz8WueIKvh8yJLLtjyQbNjidp02bRrs1uQf5cd0YY2IBjAZwLYDGAG41xjQOuFk8LCLNRKQZgLcAfO49tgKAfwFoA6A1gH8ZY8rn/baUAx4PlxHujA0GY9ixs2KFM2AEcDq79uwBPv88Kk0rkERiwuTCwKxZTuIwey3tMtB1AzhW7Y030lK3ZbY8L2GR5wt16vAzi2R+bc5ngvHRtwaQIiKbReQsgKkAumez/60APva+7wzgBxE5JCKHAfwAPh2Eh7Q0Lgug0AOMzilbli6eatWYxS4lhT7OCy90ohXyw4oVfHwuW9b/Zd1Fw4cDt96acz1paYxTHj8+8+3JyYwmKFuWboFQY4V+377wzbrjNhYvZtz6vn3O93PTTfytXXCBkwI3GKF/8UVmmgT427TlNnrFjRQtypmsgMIp9NUB7PBZ3+kty4AxpjaARAA/5uZYY8zdxpilxpil++3Ig7xQwIW+VCngv/+lP3P3bvoLU1I48UDnzhw2LflM0zNzJv/o/fpR3Pv3pxU2eTI7oUaNYiia7XDKiq++YujZc885l9WXV19lea1arC/U2KH2IrxWSs78+CMHPo0dy+/Hhvq9/z6/J1+LPiYmcz/3I4/wN9qwIVMbjB/PxGX9+zNHTcOGkf1Mkcbe0ILtiD1fCHXUTW8A00UkV7kcRWSsiLQSkVYJCQl5P7tNIVlAhR5gZ9K779LCWrKEIw7to/HRo04GzrxiRy6+8Qbwn//w9f777CDu3p2dvkWKAKNHZ1/P22/Twtm+3X+uXIB5Qr74gpEP/fpxgoi9e/PXbl/OneNN76KLuK7um+CwbsC33nK+n//8h1EzvgmzUlP51JfZ36RhQ+aYB9g5O2AA31epAtxzj3/uGTdihd5tFn0wipgKoKbPeg1vWWb0BuAbYJQKoGPAsfOCb14uKeAWvcVmwpw5k6Fv9eoxLBPgn9U3iVJuEOHNo1Mn//Jq1fgIP20a0L49B2988AEjDzK7VIcO0Tp87jlah6+84v8H//JLdocMGkTRB2iBX3ddzm08coRPCsZwME2pUhn3Wb2aoYA33kgXggo9o8rmz3e6oWrU8E/aBfC3U6IEn+iMAe6919lWvbqTmTI11X1CFircKvTBRNzEAdgMumSKAvgdQJNM9rsIwFYAxqesAoAtAMp7X1sAVMjufPmKutm7l93m77yT9zoixL/+5UQ1fPutM5T6gw/yXueOHazjrbcybluwgNEE06czJUNMTMboCt9XiRIi+/Y5aRwCXzfcwHr//JN1PfNMcG20w9cBDhXPjJde4vZff+Xy9dfzdj3cxOOP+1//2FiRbdv896lalUP469fnwCBfRozgcadPM/qre/eINf28Yu5c/p43bYp2S3IP8pMCQUTSjDFDAHwHIBbARBFZY4wZ4a14hnfX3gCmek9ojz1kjHkOgM1GPUJE8umcyAZr0UdhwFRu8Y1eqFePFpoxGZNK5QbbgZlZh1m7drTkqlbl+qZN2Q/SqlSJ4xIeeYT9B4ETq1hfbcmSTCwVzMCmvXuZ4KlfP1qokydzCL5vsqf0dLq2OnakxRofrxY9wI77OnUYmXX4MJ/axoxh6g2AT4a7d7O/54036HbzxXdO0tRUpsxVMtKxI69jpcgPxQkrQfk4RGQmgJkBZcMD1p/N4tiJACbmsX254zxx3QCOGMfG0m1TpAhH1+VX6IsU4QCXzLAiD3DgVjDExmYcCRlI69bMACiSvQ93/HjmAxk2jGL1+ed0Ifm6GGbOpM//tddYVzhn3Tmf2LWLPvPmzbl+/fWcwGL4cN4MbXbJevWAcuUyHm+FPiWF1951rokQ4jaRB1w2w9T5JPQJCfzj1qnjTHpQr17GyYp/+omTHPTowdfjj/tH5rz4orNtyhSKfLFiEfsYACj0hw+zs9e2pUcP/0Ru6emM2ujUiU8DbdrQYh89mp9n9WpOYvHQQ3y6saF9VugnTWIfQ36x5+nRA/j++7zVMW9ezumn163jRDWZRSzlhUC/+pAhHOH66adc9xX6zLDH2icvt0WVKDmQlU8nWq98+ejt0LYPP8x7HRHk9ddFXn7ZWR84UCQhwVn3eERatmSipSZNROrW5cebO5fbbTKpmjW5vUkTTvYQaXbsYJIo24YmTUQqVmQyrPR07vPbb2zrlCnOcTZ51Ny5IjfdJFKsGI+dPNnZ55ZbOGq4eHGRChVETp7MX1vtecqVE7noIv9RysHSpQsTWJ0+nf15ACa8yi/p6Tzf0KFOmccj0rChSJs2XLd9KVklzzt0iNvtb2jFivy3SylYIBsffdSFPfCVL6G3iaQ/+STvdUSRl19m8+3sPYsXc330aK6fPEmx69GD6w88wNl99u6NTnuzw84GtGED1+1sQ74ZAe3nufxydoD5Cpnl4Yf9OyEnTcp7m7ZvZyfm0KEi77/P+mbPzl0dHg/bDDDPenbnAZjVNL/YGIPAHO5vvil/ZZO8+27eXLNrd7Fi3L9du/y3SSl4ZCf06ropQNjH7jVrmPHyzTfZUWnjmosX5zyaX3zBvNmTJwO33FIwfYq++b8Bhl9WqODfN2A/z4IFXB80KGM91uXQvj3QuDFjxE+ezPgKzAoayKlTnBdAhOe55RagYkVe48zqy+zl8TCdgB3r4DvhtS9jxnDfgQOZzM534vjMEMn+M2Q1krVvX4anvvUW88tnl+PdZn4E6PZRChlZ3QGi9cqXRb9sGU0W34Tw5xF2Bh3f1/33+++zZYszByfAOSoLImlpnHXItv+SS+jyCMR+nm7dMq9n2jR+zqlTGTWbVTioMVl/7dbyBfzPY+cvDfbVrp3IRx/JX+GNgTMtidCdk5DA8+zdyyeuRx7hts6dM5/8rFcvp87Mvk87c5NvnnjLffc57QvMLx/IlVeKVK6syfPcCgrLDFPnw8jY7GjSBJgwgTPdA/wYgbPF2xC7jRvZodamTcSbGRSxsexstfNwrl7tPyOQpU4d4JtvGKKZGd26Mf1rz558YEtPzzjjD8BO3VGjuL8v6eksb9aMcwL07OlsGzqUneLB5NJZu5YjjD0ePol07Jh5SOn06ewkHTyYT1p/+xufWE6cYMqLuXMZKWOfwjZvZidz9+7AjBnA7NlA27b+dWaXm+bZZ5mLJj2dA8yy4403+NQQGHqpFAKyugNE65Uvi37hQpo2332X9zqUkPHoo7RoZ8/m1/L11+E7l50HNLCT8X//Y3lmKXlzw59/cj5VgH0Kzz8vfv0plrZtOaeo7YS212DOHMfyHjnS2f/RR2nJ79wp0qiRyPXXZzz3M8/wieXcufx9BsXdoNBY9Oe5j95ttG7NuHk7qCecmQ/vugt45hnG6NvQTACYOJHhmoGWfm4pWZLneP11fi474G3kSKff4fBhphl44w0nxa+9BjYLaLNm7CuoUIHrEyYwPUX16tx31izeDqZPBw4e5JNOdrlpFCUosroDROuVL4v+xx9pMs2bl/c6lJCRmsp5fIHITOQweHDmvvVXXw1N/Zs2MTXEzJm05EuXzniuChX8rfytW1lepIhI7doi33yTsW9hwQLuO3o0y2zEEsCw0jZtGGarKNkBteiVaFCtGv3VJ04w1W24efNNzsvrS0wM/fChoF49JmWzA9x27QL+/NN/nzJlmFjMUqsW/fH79tFi79qVlvrZs9xerJgzktU+JTz6KNs8dSpw9dVMVJffJxKlcOMuRVShL3CULu2fyyacZJVjPZRYkQcY2phZ9k1fbKbSr792XFfWbRPIJZewo/TwYeDJJzlBTfv2nFdYUxYo+UHj6BUlzFiBz2kavqJF6cOPiWHud4DRO4AKvZI/3KWIKvRKAeT229mhGhg2mRmPP85BWXZKu5tu4ixRmYWmKkqwuEsRVeiVAkjduhwtGww33+y/XrRozrOBKUpOqOtGURTF5ajQK4qiuBx3Cf15ngJBURQlHLhL6NWiVxRFyYA7hf48mDNWURQlUrhT6NWiVxRF+QsVekVRFJejQq8oiuJyVOgVRVFcjgq9oiiKy3Gn0GvUjaIoyl+4T+hjY5kbVlEURQHgNqFPT1e3jaIoSgDuEnpr0SuKoih/4T6hV4teURTFDxV6RVEUl6NCryiK4nJU6BVFUVxOUEJvjOlijNlgjEkxxgzLYp9bjDFrjTFrjDEf+ZSnG2NWeF8zQtXwTFGhVxRFyUCOqmiMiQUwGkAnADsB/GqMmSEia332qQ/gCQCXi8hhY0wlnypOiUiz0DY7C1ToFUVRMhCMRd8aQIqIbBaRswCmAugesM9AAKNF5DAAiMi+0DYzSFToFUVRMhCM0FcHsMNnfae3zJcGABoYYxYYYxYbY7r4bCtmjFnqLb8hf83NARV6RVGUDIRKFeMA1AfQEUANAD8ZY5qKyBEAtUUk1RhTF8CPxphVIvKH78HGmLsB3A0AtWrVynsrVOgVRVEyEIxFnwqgps96DW+ZLzsBzBCRcyKyBcBGUPghIqne5WYA8wA0DzyBiIwVkVYi0iohISHXH+Iv0tN1ZKyiKEoAwQj9rwDqG2MSjTFFAfQGEBg98z/QmocxpiLoytlsjClvjIn3Kb8cwFqEC7XoFUVRMpCjKopImjFmCIDvAMQCmCgia4wxIwAsFZEZ3m3XGGPWAkgH8JiIHDTGtAMwxhjjAW8qL/lG64QcFXpFUZQMBKWKIjITwMyAsuE+7wXAI96X7z4LATTNfzODRIVeURQlAzoyVlEUxeWo0CuKorgcFXpFURSXo0KvKIriclToFUVRXI4KvaIoistxl9DryFhFUZQMuEvo1aJXFEXJgAq9oiiKy1GhVxRFcTkq9IqiKC5HhV5RFMXlqNAriqK4HBV6RVEUl6NCryiK4nLcJ/Q6YEpRFMUP9wi9CODxqEWvKIoSgHuEPj2dSxV6RVEUP9wj9GlpXKrQK4qi+KFCryiK4nJU6BVFUVyOCr2iKIrLcY/Qx8UBPXsC9etHuyWKoigFCveYv+XKAdOmRbsViqIoBQ73WPSKoihKpqjQK4qiuBwVekVRFJejQq8oiuJyVOgVRVFcjgq9oiiKy1GhVxRFcTkq9IqiKC7HiEi02+CHMWY/gG35qKIigAMhak4o0XbljoLaLqDgtk3blTsKaruAvLWttogkZLahwAl9fjHGLBWRVtFuRyDartxRUNsFFNy2abtyR0FtFxD6tqnrRlEUxeWo0CuKorgcNwr92Gg3IAu0XbmjoLYLKLht03bljoLaLiDEbXOdj15RFEXxx40WvaIoiuKDCr2iKIrLcY3QG2O6GGM2GGNSjDHDotiOmsaYucaYtcaYNcaYB73lzxpjUo0xK7yvrlFq31ZjzCpvG5Z6yyoYY34wxmzyLstHuE0Nfa7LCmPMMWPMQ9G4ZsaYicaYfcaY1T5lmV4fQ970/uZWGmNaRLhdrxpj1nvP/YUxppy3vI4x5pTPdXsvXO3Kpm1ZfnfGmCe812yDMaZzhNv1iU+bthpjVnjLI3bNstGI8P3OROS8fwGIBfAHgLoAigL4HUDjKLWlKoAW3velAWwE0BjAswAeLQDXaiuAigFlrwAY5n0/DMDLUf4u9wCoHY1rBqADgBYAVud0fQB0BfAtAAOgLYAlEW7XNQDivO9f9mlXHd/9onTNMv3uvP+F3wHEA0j0/m9jI9WugO2vARge6WuWjUaE7XfmFou+NYAUEdksImcBTAXQPRoNEZHdIrLc+/44gHUAqkejLbmgO4D3ve/fB3BD9JqCqwH8ISL5GR2dZ0TkJwCHAoqzuj7dAfxXyGIA5YwxVSPVLhH5XkTSvKuLAdQIx7lzIotrlhXdAUwVkTMisgVACvj/jWi7jDEGwC0APg7HubMjG40I2+/MLUJfHcAOn/WdKADiaoypA6A5gCXeoiHeR6+JkXaP+CAAvjfGLDPG3O0tqywiu73v9wCoHJ2mAQB6w//PVxCuWVbXpyD97vqDVp8l0RjzmzFmvjHmiii1KbPvrqBcsysA7BWRTT5lEb9mARoRtt+ZW4S+wGGMKQXgMwAPicgxAO8CuBBAMwC7wcfGaNBeRFoAuBbAYGNMB9+NwmfFqMTcGmOKAugG4FNvUUG5Zn8RzeuTFcaYpwCkAfjQW7QbQC0RaQ7gEQAfGWPKRLhZBe67C+BW+BsUEb9mmWjEX4T6d+YWoU8FUNNnvYa3LCoYY4qAX+CHIvI5AIjIXhFJFxEPgHEI0+NqTohIqne5D8AX3nbstY+C3uW+aLQNvPksF5G93jYWiGuGrK9P1H93xph+AK4D0McrDvC6RQ563y8D/eANItmubL67gnDN4gDcBOATWxbpa5aZRiCMvzO3CP2vAOobYxK9VmFvADOi0RCv728CgHUi8m+fcl+f2o0AVgceG4G2lTTGlLbvwc681eC16uvdrS+ALyPdNi9+VlZBuGZesro+MwDc6Y2KaAvgqM+jd9gxxnQB8DiAbiJy0qc8wRgT631fF0B9AJsj1S7vebP67mYA6G2MiTfGJHrblhzJtgH4PwDrRWSnLYjkNctKIxDO31kkepkj8QJ7pjeCd+KnotiO9uAj10oAK7yvrgCmAFjlLZ8BoGoU2lYXjHj4HcAae50AXABgDoBNAGYDqBCFtpUEcBBAWZ+yiF8z8EazG8A50Bc6IKvrA0ZBjPb+5lYBaBXhdqWAvlv7O3vPu+/N3u93BYDlAK6PwjXL8rsD8JT3mm0AcG0k2+UtnwxgUMC+Ebtm2WhE2H5nmgJBURTF5bjFdaMoiqJkgQq9oiiKy1GhVxRFcTkq9IqiKC5HhV5RFMXlqNAriqK4HBV6RVEUl/P/Ab0ZRz5oovS4AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "epochs_list = list(range(history.params['epochs']))\n",
    "plt.plot(epochs_list, history.history['loss'], color='red')\n",
    "plt.plot(epochs_list, history.history['val_loss'], color='blue')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs_list, history.history['binary_accuracy'], color='red')\n",
    "plt.plot(epochs_list, history.history['val_binary_accuracy'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4dcc874",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ee677ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.7951582867783985\n",
      "\n",
      "confusion matrix:\n",
      "[[307  35]\n",
      " [ 75 120]]\n",
      "\n",
      "FNR: 0.19633507853403143\n",
      "FPR: 0.22580645161290322\n",
      "\n",
      "auc: 0.864432448642975\n",
      "\n",
      "precision: 0.7741935483870968\n",
      "recall: 0.6153846153846154\n",
      "fscore: 0.6857142857142857\n",
      "\n",
      "-------------------\n",
      "validation metrics\n",
      "-------------------\n",
      "accuracy (at threshold 0.5): 0.7304347826086957\n",
      "\n",
      "confusion matrix:\n",
      "[[66 14]\n",
      " [17 18]]\n",
      "\n",
      "FNR: 0.20481927710843373\n",
      "FPR: 0.4375\n",
      "\n",
      "auc: 0.7646428571428572\n",
      "\n",
      "precision: 0.5625\n",
      "recall: 0.5142857142857142\n",
      "fscore: 0.5373134328358209\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.75\n",
      "\n",
      "confusion matrix:\n",
      "[[65 13]\n",
      " [16 22]]\n",
      "\n",
      "FNR: 0.19753086419753085\n",
      "FPR: 0.37142857142857144\n",
      "\n",
      "auc: 0.8292847503373819\n",
      "\n",
      "precision: 0.6285714285714286\n",
      "recall: 0.5789473684210527\n",
      "fscore: 0.6027397260273972\n"
     ]
    }
   ],
   "source": [
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "validation_predicted_probabilites = model.predict(validation_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)\n",
    "\n",
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('-------------------')\n",
    "print('validation metrics')\n",
    "print('-------------------')\n",
    "print_metrics(validation_predicted_probabilites[:, 1], validation_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b0ce92",
   "metadata": {},
   "source": [
    "## With Dropout and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a769cee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 1024)              9216      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 2050      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,266\n",
      "Trainable params: 11,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(keras.Input(shape=(8,))) \n",
    "model.add(keras.layers.Dense(1024, activation=None, kernel_regularizer=keras.regularizers.L2(l2=1e-4)))\n",
    "model.add(keras.layers.LeakyReLU(alpha=0.05))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(2, activation='sigmoid'))\n",
    " \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9d67b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "68/68 [==============================] - 1s 4ms/step - loss: 0.6675 - binary_accuracy: 0.6322 - val_loss: 0.6469 - val_binary_accuracy: 0.7087\n",
      "Epoch 2/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.6229 - binary_accuracy: 0.7272 - val_loss: 0.6160 - val_binary_accuracy: 0.7174\n",
      "Epoch 3/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.5843 - binary_accuracy: 0.7523 - val_loss: 0.5968 - val_binary_accuracy: 0.7391\n",
      "Epoch 4/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5620 - binary_accuracy: 0.7598 - val_loss: 0.5818 - val_binary_accuracy: 0.7304\n",
      "Epoch 5/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.5405 - binary_accuracy: 0.7654 - val_loss: 0.5697 - val_binary_accuracy: 0.7261\n",
      "Epoch 6/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5217 - binary_accuracy: 0.7793 - val_loss: 0.5602 - val_binary_accuracy: 0.7348\n",
      "Epoch 7/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5110 - binary_accuracy: 0.7775 - val_loss: 0.5551 - val_binary_accuracy: 0.7348\n",
      "Epoch 8/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5024 - binary_accuracy: 0.7812 - val_loss: 0.5502 - val_binary_accuracy: 0.7304\n",
      "Epoch 9/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4939 - binary_accuracy: 0.7784 - val_loss: 0.5465 - val_binary_accuracy: 0.7304\n",
      "Epoch 10/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4833 - binary_accuracy: 0.7886 - val_loss: 0.5447 - val_binary_accuracy: 0.7348\n",
      "Epoch 11/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4770 - binary_accuracy: 0.7737 - val_loss: 0.5448 - val_binary_accuracy: 0.7348\n",
      "Epoch 12/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4662 - binary_accuracy: 0.7933 - val_loss: 0.5431 - val_binary_accuracy: 0.7348\n",
      "Epoch 13/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4699 - binary_accuracy: 0.7914 - val_loss: 0.5410 - val_binary_accuracy: 0.7261\n",
      "Epoch 14/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4658 - binary_accuracy: 0.7952 - val_loss: 0.5419 - val_binary_accuracy: 0.7261\n",
      "Epoch 15/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4607 - binary_accuracy: 0.7942 - val_loss: 0.5431 - val_binary_accuracy: 0.7304\n",
      "Epoch 16/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4584 - binary_accuracy: 0.7905 - val_loss: 0.5425 - val_binary_accuracy: 0.7304\n",
      "Epoch 17/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4556 - binary_accuracy: 0.7998 - val_loss: 0.5436 - val_binary_accuracy: 0.7217\n",
      "Epoch 18/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4544 - binary_accuracy: 0.8035 - val_loss: 0.5431 - val_binary_accuracy: 0.7217\n",
      "Epoch 19/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4512 - binary_accuracy: 0.7961 - val_loss: 0.5444 - val_binary_accuracy: 0.7174\n",
      "Epoch 20/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4475 - binary_accuracy: 0.7914 - val_loss: 0.5443 - val_binary_accuracy: 0.7174\n",
      "Epoch 21/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4464 - binary_accuracy: 0.7961 - val_loss: 0.5448 - val_binary_accuracy: 0.7217\n",
      "Epoch 22/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4454 - binary_accuracy: 0.7989 - val_loss: 0.5453 - val_binary_accuracy: 0.7217\n",
      "Epoch 23/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4485 - binary_accuracy: 0.7989 - val_loss: 0.5452 - val_binary_accuracy: 0.7261\n",
      "Epoch 24/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4467 - binary_accuracy: 0.7933 - val_loss: 0.5466 - val_binary_accuracy: 0.7174\n",
      "Epoch 25/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4443 - binary_accuracy: 0.7998 - val_loss: 0.5481 - val_binary_accuracy: 0.7174\n",
      "Epoch 26/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4394 - binary_accuracy: 0.7998 - val_loss: 0.5493 - val_binary_accuracy: 0.7174\n",
      "Epoch 27/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4394 - binary_accuracy: 0.8007 - val_loss: 0.5503 - val_binary_accuracy: 0.7043\n",
      "Epoch 28/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4394 - binary_accuracy: 0.8063 - val_loss: 0.5511 - val_binary_accuracy: 0.7087\n",
      "Epoch 29/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4334 - binary_accuracy: 0.8082 - val_loss: 0.5514 - val_binary_accuracy: 0.7087\n",
      "Epoch 30/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4394 - binary_accuracy: 0.7924 - val_loss: 0.5512 - val_binary_accuracy: 0.7043\n",
      "Epoch 31/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4329 - binary_accuracy: 0.8017 - val_loss: 0.5518 - val_binary_accuracy: 0.7000\n",
      "Epoch 32/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4354 - binary_accuracy: 0.8110 - val_loss: 0.5506 - val_binary_accuracy: 0.7087\n",
      "Epoch 33/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4327 - binary_accuracy: 0.8082 - val_loss: 0.5520 - val_binary_accuracy: 0.7043\n",
      "Epoch 34/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4326 - binary_accuracy: 0.8035 - val_loss: 0.5529 - val_binary_accuracy: 0.7043\n",
      "Epoch 35/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4295 - binary_accuracy: 0.8026 - val_loss: 0.5527 - val_binary_accuracy: 0.7000\n",
      "Epoch 36/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.4314 - binary_accuracy: 0.7989 - val_loss: 0.5516 - val_binary_accuracy: 0.7000\n",
      "Epoch 37/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4345 - binary_accuracy: 0.8054 - val_loss: 0.5523 - val_binary_accuracy: 0.7000\n",
      "Epoch 38/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4306 - binary_accuracy: 0.8063 - val_loss: 0.5525 - val_binary_accuracy: 0.6957\n",
      "Epoch 39/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4332 - binary_accuracy: 0.7998 - val_loss: 0.5522 - val_binary_accuracy: 0.6913\n",
      "Epoch 40/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4324 - binary_accuracy: 0.8045 - val_loss: 0.5528 - val_binary_accuracy: 0.6913\n",
      "Epoch 41/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4290 - binary_accuracy: 0.8017 - val_loss: 0.5530 - val_binary_accuracy: 0.6913\n",
      "Epoch 42/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4265 - binary_accuracy: 0.8017 - val_loss: 0.5535 - val_binary_accuracy: 0.6913\n",
      "Epoch 43/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4275 - binary_accuracy: 0.8119 - val_loss: 0.5544 - val_binary_accuracy: 0.6913\n",
      "Epoch 44/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4249 - binary_accuracy: 0.8101 - val_loss: 0.5547 - val_binary_accuracy: 0.6913\n",
      "Epoch 45/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4293 - binary_accuracy: 0.8026 - val_loss: 0.5549 - val_binary_accuracy: 0.6957\n",
      "Epoch 46/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4247 - binary_accuracy: 0.8110 - val_loss: 0.5558 - val_binary_accuracy: 0.6870\n",
      "Epoch 47/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4289 - binary_accuracy: 0.8054 - val_loss: 0.5561 - val_binary_accuracy: 0.6870\n",
      "Epoch 48/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4220 - binary_accuracy: 0.8119 - val_loss: 0.5558 - val_binary_accuracy: 0.6913\n",
      "Epoch 49/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4259 - binary_accuracy: 0.8054 - val_loss: 0.5582 - val_binary_accuracy: 0.6826\n",
      "Epoch 50/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4266 - binary_accuracy: 0.8082 - val_loss: 0.5583 - val_binary_accuracy: 0.6826\n",
      "Epoch 51/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4257 - binary_accuracy: 0.8101 - val_loss: 0.5584 - val_binary_accuracy: 0.6870\n",
      "Epoch 52/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4278 - binary_accuracy: 0.8054 - val_loss: 0.5584 - val_binary_accuracy: 0.6913\n",
      "Epoch 53/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4246 - binary_accuracy: 0.8166 - val_loss: 0.5579 - val_binary_accuracy: 0.6957\n",
      "Epoch 54/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4227 - binary_accuracy: 0.8110 - val_loss: 0.5576 - val_binary_accuracy: 0.6913\n",
      "Epoch 55/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4250 - binary_accuracy: 0.8073 - val_loss: 0.5580 - val_binary_accuracy: 0.6913\n",
      "Epoch 56/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4186 - binary_accuracy: 0.8128 - val_loss: 0.5575 - val_binary_accuracy: 0.6957\n",
      "Epoch 57/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4181 - binary_accuracy: 0.8156 - val_loss: 0.5592 - val_binary_accuracy: 0.6913\n",
      "Epoch 58/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4230 - binary_accuracy: 0.8110 - val_loss: 0.5581 - val_binary_accuracy: 0.6957\n",
      "Epoch 59/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4169 - binary_accuracy: 0.8203 - val_loss: 0.5592 - val_binary_accuracy: 0.6957\n",
      "Epoch 60/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4201 - binary_accuracy: 0.8128 - val_loss: 0.5591 - val_binary_accuracy: 0.6957\n",
      "Epoch 61/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4157 - binary_accuracy: 0.8073 - val_loss: 0.5606 - val_binary_accuracy: 0.6957\n",
      "Epoch 62/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4183 - binary_accuracy: 0.8128 - val_loss: 0.5580 - val_binary_accuracy: 0.6957\n",
      "Epoch 63/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4241 - binary_accuracy: 0.8119 - val_loss: 0.5575 - val_binary_accuracy: 0.7043\n",
      "Epoch 64/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4143 - binary_accuracy: 0.8166 - val_loss: 0.5584 - val_binary_accuracy: 0.6957\n",
      "Epoch 65/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4241 - binary_accuracy: 0.8175 - val_loss: 0.5601 - val_binary_accuracy: 0.6913\n",
      "Epoch 66/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4144 - binary_accuracy: 0.8119 - val_loss: 0.5610 - val_binary_accuracy: 0.6870\n",
      "Epoch 67/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4206 - binary_accuracy: 0.8166 - val_loss: 0.5604 - val_binary_accuracy: 0.6913\n",
      "Epoch 68/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4155 - binary_accuracy: 0.8240 - val_loss: 0.5598 - val_binary_accuracy: 0.7000\n",
      "Epoch 69/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4230 - binary_accuracy: 0.8035 - val_loss: 0.5602 - val_binary_accuracy: 0.7000\n",
      "Epoch 70/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4176 - binary_accuracy: 0.8175 - val_loss: 0.5612 - val_binary_accuracy: 0.6913\n",
      "Epoch 71/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4163 - binary_accuracy: 0.8156 - val_loss: 0.5619 - val_binary_accuracy: 0.6913\n",
      "Epoch 72/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4100 - binary_accuracy: 0.8240 - val_loss: 0.5629 - val_binary_accuracy: 0.6870\n",
      "Epoch 73/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4083 - binary_accuracy: 0.8212 - val_loss: 0.5632 - val_binary_accuracy: 0.6913\n",
      "Epoch 74/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4138 - binary_accuracy: 0.8175 - val_loss: 0.5638 - val_binary_accuracy: 0.6913\n",
      "Epoch 75/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4127 - binary_accuracy: 0.8184 - val_loss: 0.5631 - val_binary_accuracy: 0.7000\n",
      "Epoch 76/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4123 - binary_accuracy: 0.8147 - val_loss: 0.5630 - val_binary_accuracy: 0.7000\n",
      "Epoch 77/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4133 - binary_accuracy: 0.8147 - val_loss: 0.5656 - val_binary_accuracy: 0.6870\n",
      "Epoch 78/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4168 - binary_accuracy: 0.8184 - val_loss: 0.5647 - val_binary_accuracy: 0.6870\n",
      "Epoch 79/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4103 - binary_accuracy: 0.8128 - val_loss: 0.5645 - val_binary_accuracy: 0.6870\n",
      "Epoch 80/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4146 - binary_accuracy: 0.8231 - val_loss: 0.5644 - val_binary_accuracy: 0.6870\n",
      "Epoch 81/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4104 - binary_accuracy: 0.8166 - val_loss: 0.5644 - val_binary_accuracy: 0.6870\n",
      "Epoch 82/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4116 - binary_accuracy: 0.8231 - val_loss: 0.5667 - val_binary_accuracy: 0.6783\n",
      "Epoch 83/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4100 - binary_accuracy: 0.8259 - val_loss: 0.5660 - val_binary_accuracy: 0.6826\n",
      "Epoch 84/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4107 - binary_accuracy: 0.8184 - val_loss: 0.5665 - val_binary_accuracy: 0.6826\n",
      "Epoch 85/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4075 - binary_accuracy: 0.8184 - val_loss: 0.5661 - val_binary_accuracy: 0.6870\n",
      "Epoch 86/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4047 - binary_accuracy: 0.8212 - val_loss: 0.5660 - val_binary_accuracy: 0.6913\n",
      "Epoch 87/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4094 - binary_accuracy: 0.8259 - val_loss: 0.5641 - val_binary_accuracy: 0.6913\n",
      "Epoch 88/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4136 - binary_accuracy: 0.8194 - val_loss: 0.5651 - val_binary_accuracy: 0.6870\n",
      "Epoch 89/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4111 - binary_accuracy: 0.8110 - val_loss: 0.5656 - val_binary_accuracy: 0.6870\n",
      "Epoch 90/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4120 - binary_accuracy: 0.8240 - val_loss: 0.5662 - val_binary_accuracy: 0.6870\n",
      "Epoch 91/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4052 - binary_accuracy: 0.8194 - val_loss: 0.5665 - val_binary_accuracy: 0.6913\n",
      "Epoch 92/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4071 - binary_accuracy: 0.8175 - val_loss: 0.5658 - val_binary_accuracy: 0.6913\n",
      "Epoch 93/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4041 - binary_accuracy: 0.8212 - val_loss: 0.5670 - val_binary_accuracy: 0.6913\n",
      "Epoch 94/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4132 - binary_accuracy: 0.8222 - val_loss: 0.5681 - val_binary_accuracy: 0.6870\n",
      "Epoch 95/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4039 - binary_accuracy: 0.8156 - val_loss: 0.5699 - val_binary_accuracy: 0.6783\n",
      "Epoch 96/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4049 - binary_accuracy: 0.8212 - val_loss: 0.5718 - val_binary_accuracy: 0.6783\n",
      "Epoch 97/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3975 - binary_accuracy: 0.8277 - val_loss: 0.5711 - val_binary_accuracy: 0.6783\n",
      "Epoch 98/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4090 - binary_accuracy: 0.8184 - val_loss: 0.5697 - val_binary_accuracy: 0.6826\n",
      "Epoch 99/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4050 - binary_accuracy: 0.8268 - val_loss: 0.5692 - val_binary_accuracy: 0.6870\n",
      "Epoch 100/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4061 - binary_accuracy: 0.8250 - val_loss: 0.5692 - val_binary_accuracy: 0.6913\n",
      "Epoch 101/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4029 - binary_accuracy: 0.8305 - val_loss: 0.5689 - val_binary_accuracy: 0.6913\n",
      "Epoch 102/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4053 - binary_accuracy: 0.8194 - val_loss: 0.5716 - val_binary_accuracy: 0.6739\n",
      "Epoch 103/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4057 - binary_accuracy: 0.8184 - val_loss: 0.5709 - val_binary_accuracy: 0.6826\n",
      "Epoch 104/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3996 - binary_accuracy: 0.8324 - val_loss: 0.5721 - val_binary_accuracy: 0.6783\n",
      "Epoch 105/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4035 - binary_accuracy: 0.8277 - val_loss: 0.5728 - val_binary_accuracy: 0.6739\n",
      "Epoch 106/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4030 - binary_accuracy: 0.8296 - val_loss: 0.5721 - val_binary_accuracy: 0.6783\n",
      "Epoch 107/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4061 - binary_accuracy: 0.8212 - val_loss: 0.5690 - val_binary_accuracy: 0.6913\n",
      "Epoch 108/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4050 - binary_accuracy: 0.8315 - val_loss: 0.5696 - val_binary_accuracy: 0.6957\n",
      "Epoch 109/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3986 - binary_accuracy: 0.8315 - val_loss: 0.5704 - val_binary_accuracy: 0.6957\n",
      "Epoch 110/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3977 - binary_accuracy: 0.8287 - val_loss: 0.5709 - val_binary_accuracy: 0.6957\n",
      "Epoch 111/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4039 - binary_accuracy: 0.8240 - val_loss: 0.5708 - val_binary_accuracy: 0.6957\n",
      "Epoch 112/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4034 - binary_accuracy: 0.8231 - val_loss: 0.5696 - val_binary_accuracy: 0.6913\n",
      "Epoch 113/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4017 - binary_accuracy: 0.8277 - val_loss: 0.5740 - val_binary_accuracy: 0.6870\n",
      "Epoch 114/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4033 - binary_accuracy: 0.8417 - val_loss: 0.5710 - val_binary_accuracy: 0.7000\n",
      "Epoch 115/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4046 - binary_accuracy: 0.8259 - val_loss: 0.5727 - val_binary_accuracy: 0.6957\n",
      "Epoch 116/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4014 - binary_accuracy: 0.8287 - val_loss: 0.5736 - val_binary_accuracy: 0.6870\n",
      "Epoch 117/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3992 - binary_accuracy: 0.8333 - val_loss: 0.5740 - val_binary_accuracy: 0.6826\n",
      "Epoch 118/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4003 - binary_accuracy: 0.8203 - val_loss: 0.5760 - val_binary_accuracy: 0.6870\n",
      "Epoch 119/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3938 - binary_accuracy: 0.8352 - val_loss: 0.5758 - val_binary_accuracy: 0.6826\n",
      "Epoch 120/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3923 - binary_accuracy: 0.8426 - val_loss: 0.5758 - val_binary_accuracy: 0.6826\n",
      "Epoch 121/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3989 - binary_accuracy: 0.8371 - val_loss: 0.5752 - val_binary_accuracy: 0.6913\n",
      "Epoch 122/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4027 - binary_accuracy: 0.8287 - val_loss: 0.5740 - val_binary_accuracy: 0.6957\n",
      "Epoch 123/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3990 - binary_accuracy: 0.8250 - val_loss: 0.5749 - val_binary_accuracy: 0.6957\n",
      "Epoch 124/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3889 - binary_accuracy: 0.8324 - val_loss: 0.5751 - val_binary_accuracy: 0.6913\n",
      "Epoch 125/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4046 - binary_accuracy: 0.8305 - val_loss: 0.5774 - val_binary_accuracy: 0.6739\n",
      "Epoch 126/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3948 - binary_accuracy: 0.8389 - val_loss: 0.5795 - val_binary_accuracy: 0.6739\n",
      "Epoch 127/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3986 - binary_accuracy: 0.8371 - val_loss: 0.5784 - val_binary_accuracy: 0.6783\n",
      "Epoch 128/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3986 - binary_accuracy: 0.8287 - val_loss: 0.5784 - val_binary_accuracy: 0.6870\n",
      "Epoch 129/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4019 - binary_accuracy: 0.8361 - val_loss: 0.5793 - val_binary_accuracy: 0.6870\n",
      "Epoch 130/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3961 - binary_accuracy: 0.8315 - val_loss: 0.5805 - val_binary_accuracy: 0.6870\n",
      "Epoch 131/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3948 - binary_accuracy: 0.8296 - val_loss: 0.5795 - val_binary_accuracy: 0.6913\n",
      "Epoch 132/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3957 - binary_accuracy: 0.8268 - val_loss: 0.5828 - val_binary_accuracy: 0.6913\n",
      "Epoch 133/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3954 - binary_accuracy: 0.8305 - val_loss: 0.5837 - val_binary_accuracy: 0.6870\n",
      "Epoch 134/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3896 - binary_accuracy: 0.8454 - val_loss: 0.5843 - val_binary_accuracy: 0.6870\n",
      "Epoch 135/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3973 - binary_accuracy: 0.8333 - val_loss: 0.5842 - val_binary_accuracy: 0.6870\n",
      "Epoch 136/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3937 - binary_accuracy: 0.8343 - val_loss: 0.5834 - val_binary_accuracy: 0.6870\n",
      "Epoch 137/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3912 - binary_accuracy: 0.8287 - val_loss: 0.5817 - val_binary_accuracy: 0.6826\n",
      "Epoch 138/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3919 - binary_accuracy: 0.8315 - val_loss: 0.5799 - val_binary_accuracy: 0.7043\n",
      "Epoch 139/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3905 - binary_accuracy: 0.8408 - val_loss: 0.5817 - val_binary_accuracy: 0.6957\n",
      "Epoch 140/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3952 - binary_accuracy: 0.8333 - val_loss: 0.5814 - val_binary_accuracy: 0.6957\n",
      "Epoch 141/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3913 - binary_accuracy: 0.8399 - val_loss: 0.5827 - val_binary_accuracy: 0.6957\n",
      "Epoch 142/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3902 - binary_accuracy: 0.8380 - val_loss: 0.5828 - val_binary_accuracy: 0.6957\n",
      "Epoch 143/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3950 - binary_accuracy: 0.8333 - val_loss: 0.5825 - val_binary_accuracy: 0.7043\n",
      "Epoch 144/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3871 - binary_accuracy: 0.8399 - val_loss: 0.5828 - val_binary_accuracy: 0.7043\n",
      "Epoch 145/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3874 - binary_accuracy: 0.8380 - val_loss: 0.5843 - val_binary_accuracy: 0.7043\n",
      "Epoch 146/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3978 - binary_accuracy: 0.8333 - val_loss: 0.5848 - val_binary_accuracy: 0.7043\n",
      "Epoch 147/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3875 - binary_accuracy: 0.8389 - val_loss: 0.5853 - val_binary_accuracy: 0.7043\n",
      "Epoch 148/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3960 - binary_accuracy: 0.8324 - val_loss: 0.5845 - val_binary_accuracy: 0.7043\n",
      "Epoch 149/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3906 - binary_accuracy: 0.8296 - val_loss: 0.5849 - val_binary_accuracy: 0.7043\n",
      "Epoch 150/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3879 - binary_accuracy: 0.8380 - val_loss: 0.5834 - val_binary_accuracy: 0.7087\n",
      "Epoch 151/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3864 - binary_accuracy: 0.8445 - val_loss: 0.5841 - val_binary_accuracy: 0.7087\n",
      "Epoch 152/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3851 - binary_accuracy: 0.8408 - val_loss: 0.5858 - val_binary_accuracy: 0.7043\n",
      "Epoch 153/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3916 - binary_accuracy: 0.8352 - val_loss: 0.5852 - val_binary_accuracy: 0.7087\n",
      "Epoch 154/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3926 - binary_accuracy: 0.8399 - val_loss: 0.5862 - val_binary_accuracy: 0.7087\n",
      "Epoch 155/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3877 - binary_accuracy: 0.8371 - val_loss: 0.5855 - val_binary_accuracy: 0.7087\n",
      "Epoch 156/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3869 - binary_accuracy: 0.8324 - val_loss: 0.5852 - val_binary_accuracy: 0.7130\n",
      "Epoch 157/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3840 - binary_accuracy: 0.8408 - val_loss: 0.5866 - val_binary_accuracy: 0.7043\n",
      "Epoch 158/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3954 - binary_accuracy: 0.8408 - val_loss: 0.5863 - val_binary_accuracy: 0.7130\n",
      "Epoch 159/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3909 - binary_accuracy: 0.8408 - val_loss: 0.5878 - val_binary_accuracy: 0.7043\n",
      "Epoch 160/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3888 - binary_accuracy: 0.8399 - val_loss: 0.5876 - val_binary_accuracy: 0.7043\n",
      "Epoch 161/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3922 - binary_accuracy: 0.8315 - val_loss: 0.5880 - val_binary_accuracy: 0.7043\n",
      "Epoch 162/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3821 - binary_accuracy: 0.8445 - val_loss: 0.5875 - val_binary_accuracy: 0.7087\n",
      "Epoch 163/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3810 - binary_accuracy: 0.8473 - val_loss: 0.5885 - val_binary_accuracy: 0.7087\n",
      "Epoch 164/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3825 - binary_accuracy: 0.8454 - val_loss: 0.5894 - val_binary_accuracy: 0.7043\n",
      "Epoch 165/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3809 - binary_accuracy: 0.8454 - val_loss: 0.5853 - val_binary_accuracy: 0.7043\n",
      "Epoch 166/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3798 - binary_accuracy: 0.8408 - val_loss: 0.5866 - val_binary_accuracy: 0.7043\n",
      "Epoch 167/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3911 - binary_accuracy: 0.8399 - val_loss: 0.5881 - val_binary_accuracy: 0.7043\n",
      "Epoch 168/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3834 - binary_accuracy: 0.8426 - val_loss: 0.5906 - val_binary_accuracy: 0.7043\n",
      "Epoch 169/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3906 - binary_accuracy: 0.8417 - val_loss: 0.5906 - val_binary_accuracy: 0.7043\n",
      "Epoch 170/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3847 - binary_accuracy: 0.8333 - val_loss: 0.5927 - val_binary_accuracy: 0.7043\n",
      "Epoch 171/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3901 - binary_accuracy: 0.8380 - val_loss: 0.5925 - val_binary_accuracy: 0.7043\n",
      "Epoch 172/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3831 - binary_accuracy: 0.8436 - val_loss: 0.5931 - val_binary_accuracy: 0.7043\n",
      "Epoch 173/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3894 - binary_accuracy: 0.8333 - val_loss: 0.5944 - val_binary_accuracy: 0.7043\n",
      "Epoch 174/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3869 - binary_accuracy: 0.8315 - val_loss: 0.5874 - val_binary_accuracy: 0.7174\n",
      "Epoch 175/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3884 - binary_accuracy: 0.8417 - val_loss: 0.5892 - val_binary_accuracy: 0.7174\n",
      "Epoch 176/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3821 - binary_accuracy: 0.8371 - val_loss: 0.5900 - val_binary_accuracy: 0.7174\n",
      "Epoch 177/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3852 - binary_accuracy: 0.8445 - val_loss: 0.5911 - val_binary_accuracy: 0.7130\n",
      "Epoch 178/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3826 - binary_accuracy: 0.8380 - val_loss: 0.5925 - val_binary_accuracy: 0.7087\n",
      "Epoch 179/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3899 - binary_accuracy: 0.8361 - val_loss: 0.5934 - val_binary_accuracy: 0.7087\n",
      "Epoch 180/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3870 - binary_accuracy: 0.8445 - val_loss: 0.5972 - val_binary_accuracy: 0.7043\n",
      "Epoch 181/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3769 - binary_accuracy: 0.8501 - val_loss: 0.5975 - val_binary_accuracy: 0.7043\n",
      "Epoch 182/200\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.3821 - binary_accuracy: 0.8445 - val_loss: 0.5937 - val_binary_accuracy: 0.7000\n",
      "Epoch 183/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3818 - binary_accuracy: 0.8426 - val_loss: 0.5940 - val_binary_accuracy: 0.7000\n",
      "Epoch 184/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8464 - val_loss: 0.5958 - val_binary_accuracy: 0.7043\n",
      "Epoch 185/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3874 - binary_accuracy: 0.8464 - val_loss: 0.5982 - val_binary_accuracy: 0.7087\n",
      "Epoch 186/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3864 - binary_accuracy: 0.8343 - val_loss: 0.5988 - val_binary_accuracy: 0.7000\n",
      "Epoch 187/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3802 - binary_accuracy: 0.8408 - val_loss: 0.5981 - val_binary_accuracy: 0.6957\n",
      "Epoch 188/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3798 - binary_accuracy: 0.8482 - val_loss: 0.5996 - val_binary_accuracy: 0.6957\n",
      "Epoch 189/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3840 - binary_accuracy: 0.8315 - val_loss: 0.5975 - val_binary_accuracy: 0.7000\n",
      "Epoch 190/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3823 - binary_accuracy: 0.8426 - val_loss: 0.5986 - val_binary_accuracy: 0.7000\n",
      "Epoch 191/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3836 - binary_accuracy: 0.8454 - val_loss: 0.5982 - val_binary_accuracy: 0.7000\n",
      "Epoch 192/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3764 - binary_accuracy: 0.8436 - val_loss: 0.5982 - val_binary_accuracy: 0.7000\n",
      "Epoch 193/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3810 - binary_accuracy: 0.8399 - val_loss: 0.5997 - val_binary_accuracy: 0.7000\n",
      "Epoch 194/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3774 - binary_accuracy: 0.8408 - val_loss: 0.5995 - val_binary_accuracy: 0.7000\n",
      "Epoch 195/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3938 - binary_accuracy: 0.8445 - val_loss: 0.5994 - val_binary_accuracy: 0.7000\n",
      "Epoch 196/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3825 - binary_accuracy: 0.8343 - val_loss: 0.5999 - val_binary_accuracy: 0.7000\n",
      "Epoch 197/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3748 - binary_accuracy: 0.8436 - val_loss: 0.6007 - val_binary_accuracy: 0.7000\n",
      "Epoch 198/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3851 - binary_accuracy: 0.8389 - val_loss: 0.6010 - val_binary_accuracy: 0.7000\n",
      "Epoch 199/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3772 - binary_accuracy: 0.8389 - val_loss: 0.6018 - val_binary_accuracy: 0.7000\n",
      "Epoch 200/200\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.3837 - binary_accuracy: 0.8305 - val_loss: 0.6042 - val_binary_accuracy: 0.7000\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', \n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.01),\n",
    "             metrics=['binary_accuracy'])\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model.hdf5',\n",
    "    monitor='val_binary_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "history = model.fit(train_data_x, \n",
    "          train_data_y, \n",
    "          validation_data=(validation_data_x, validation_data_y),\n",
    "          batch_size=8, \n",
    "          epochs=200,\n",
    "          callbacks=[model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "588cf7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs_list = list(range(history.params['epochs']))\n",
    "plt.plot(epochs_list, history.history['loss'], color='red')\n",
    "plt.plot(epochs_list, history.history['val_loss'], color='blue')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(epochs_list, history.history['binary_accuracy'], color='red')\n",
    "plt.plot(epochs_list, history.history['val_binary_accuracy'], color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f53d433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 0s 1ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "----------------\n",
      "training metrics\n",
      "----------------\n",
      "accuracy (at threshold 0.5): 0.776536312849162\n",
      "\n",
      "confusion matrix:\n",
      "[[313  29]\n",
      " [ 91 104]]\n",
      "\n",
      "FNR: 0.22524752475247525\n",
      "FPR: 0.21804511278195488\n",
      "\n",
      "auc: 0.8554355975408606\n",
      "\n",
      "precision: 0.7819548872180451\n",
      "recall: 0.5333333333333333\n",
      "fscore: 0.6341463414634146\n",
      "\n",
      "-------------------\n",
      "validation metrics\n",
      "-------------------\n",
      "accuracy (at threshold 0.5): 0.7478260869565218\n",
      "\n",
      "confusion matrix:\n",
      "[[69 11]\n",
      " [18 17]]\n",
      "\n",
      "FNR: 0.20689655172413793\n",
      "FPR: 0.39285714285714285\n",
      "\n",
      "auc: 0.7664285714285715\n",
      "\n",
      "precision: 0.6071428571428571\n",
      "recall: 0.4857142857142857\n",
      "fscore: 0.5396825396825397\n",
      "\n",
      "------------\n",
      "test metrics\n",
      "------------\n",
      "accuracy (at threshold 0.5): 0.7758620689655172\n",
      "\n",
      "confusion matrix:\n",
      "[[70  8]\n",
      " [18 20]]\n",
      "\n",
      "FNR: 0.20454545454545456\n",
      "FPR: 0.2857142857142857\n",
      "\n",
      "auc: 0.8630229419703104\n",
      "\n",
      "precision: 0.7142857142857143\n",
      "recall: 0.5263157894736842\n",
      "fscore: 0.6060606060606061\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('model.hdf5')\n",
    "\n",
    "train_predicted_probabilites = model.predict(train_data_x)\n",
    "validation_predicted_probabilites = model.predict(validation_data_x)\n",
    "test_predicted_probabilities = model.predict(test_data_x)\n",
    "\n",
    "print('----------------')\n",
    "print('training metrics')\n",
    "print('----------------')\n",
    "print_metrics(train_predicted_probabilites[:, 1], train_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('-------------------')\n",
    "print('validation metrics')\n",
    "print('-------------------')\n",
    "print_metrics(validation_predicted_probabilites[:, 1], validation_data_y[:, 1])\n",
    "\n",
    "print('')\n",
    "print('------------')\n",
    "print('test metrics')\n",
    "print('------------')\n",
    "print_metrics(test_predicted_probabilities[:, 1], test_data_y[:, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
